\documentclass[a4paper,10pt]{scrartcl}
\usepackage{ngerman}
\usepackage{amsmath, amsfonts, epsfig, xspace}
\usepackage[utf8x]{inputenc}
\usepackage[english]{babel}
% For graphics
\usepackage[pdftex]{graphicx}
% you can place a figure at the position where it occurs in the text using [H]
\usepackage{here}
% For flexible tables
\usepackage{multirow}
% Math symbols not defined in the usual package, e.g. arrows that are crossed.
\usepackage{amssymb}

\usepackage{pbox}

% Arrows with text / superscript
\usepackage{amsmath}
% Adjust margin of paper.
\usepackage{geometry}
\geometry{a4paper, top=25mm, left=25mm, right=25mm, bottom=25mm}

% Zeilenabstand 1.25 %
\linespread{1.2}

\newcommand*{\SignatureAndDate}[2]{%
    \par\noindent\makebox[2.5in]{\hrulefill} \hfill\makebox[2.0in]{\hrulefill}%
    \par\noindent\makebox[2.5in][l]{#1}      \hfill\makebox[2.0in][l]{#2}%
}%

% Example Environments
\usepackage{amsthm}
\newtheoremstyle{style}   
  {0.5cm}              %Space above    
  {-0.8cm}              %Space below
  {}                      %Body font: original {\normalfont}    
  {}                      %Indent amount (empty = no indent,%\parindent = paraindent)    
  {\normalfont\bfseries}  %Thm head font original       
  {{\normalfont\bfseries \thmname{#1}\thmnumber{ #2}}}
\theoremstyle{style}
\newtheorem{example}{Example}[section]

% Formula Environments
\newtheorem{formula}{Formula}[section]

% Computational Linguistics trees etc.
\usepackage{xyling}

% Nicer captions
\usepackage{caption2}
\newcaptionstyle{mystyle}{%
  \normalcaptionparams
  \renewcommand\captionlabelfont{\bfseries}%
  \renewcommand\captionlabeldelim{.}%
  \onelinecaptionsfalse
  \usecaptionstyle{centerlast}}

\captionstyle{mystyle}

% Table of contents depth
\setcounter{tocdepth}{3}

% A horizontal rule for the title page
\newcommand{\HRule}{\rule{\linewidth}{0.5mm}}

% Paragraph and indent (as required by Prof. Dr. Pinkal)
\setlength{\parindent}{0pt}
\setlength{\parskip}{2ex plus 0.5ex minus 0.2ex}


\usepackage{color}

\usepackage{listings} %regex (listings)
\lstset{
    breaklines     = true,
    stepnumber     = 5,
    stringstyle    = \ttfamily,
    showstringspaces=true
}

\usepackage{rotating} %landscape tables
\usepackage{natbib} %schönere Zitate

\usepackage[colorinlistoftodos,textsize=tiny]{todonotes}

\presetkeys{todonotes}{fancyline}{}
\definecolor{todoorange}{rgb}{1, 0.8, 0.4}

\newcommand{\comment}[1]{
\todo[bordercolor=todoorange!80!black,color=todoorange]{\textbf{Comment:} #1}
}

\newcommand{\cn}{$^\text{[citation needed] }$}

\renewcommand\abstractname{\textbf{Abstract}}

\author{Jonathan Oberländer}

\title{Automatic Detection of Linguistic Quality Violations}

\begin{document}

\input{title.tex}

\thispagestyle{empty}
\section*{Erklärung}
Hiermit versichere ich, dass ich die vorliegende Arbeit selbstständig verfasst und keine anderen als die angegebenen Quellen und Hilfsmittel benutzt habe, dass alle Stellen der Arbeit, die wörtlich oder sinngemäß aus anderen Quellen übernommen wurden, als solche kenntlich gemacht und dass die Arbeit in gleicher oder ähnlicher Form noch keiner Prüfungsbehörde vorgelegt wurde.
\vspace{2cm}

\SignatureAndDate{Ort, Datum}{Unterschrift}

\vspace{5cm}

\section*{Declaration}

I hereby confirm that the thesis presented here is my own work, with all assistance acknowledged.
\vspace{2cm}

\SignatureAndDate{Place, Date}{Signature}
\newpage

\thispagestyle{empty}
\begin{abstract}
\setlength{\parskip}{2ex plus 0.5ex minus 0.2ex}
\textbf{Abstract} \quad Automatic summarization systems are usually evaluated on content coverage rather than linguistic quality factors, which leads to summaries containing many linguistic quality violations, like ungrammatical sentences.
Building on the work of \cite{friedrichlqvsumm}, who annotated automatic summaries with a set of violation types, we first find a number of subtypes of ungrammaticality, and then we develop methods for detecting the clause-level violations ungrammaticality, inclusion of datelines and redundancy automatically.
We obtain F-Scores of 58.6\% for ungrammaticality, 87.8\% for redundancy and 37.0\% for redundancy.
\end{abstract}

\section*{Acknowledgements}
I would like to thank my supervisors Prof. Manfred Pinkal, Annemarie Friedrich and Alexis Palmer for their continuous support in the process of creating this thesis, and for providing me with helpful comments and guidance whenever I asked for it.

This work couldn't have been done without the work previously done by Marina Valeeva.

I would also like to thank Lukas Burk for helping me with R \citep{R} and ggplot2 \citep{ggplot2}.
\newpage

% Table of contents
\thispagestyle{empty}
\tableofcontents
\newpage

% Start of content
\setcounter{page}{1}		% Seitenzähler auf 1 setzen %
%\pagestyle{fancy}				% fancy header style
\pagenumbering{arabic}
\newpage

% precision/recall/…: umgekehrt ebenfalls, d.h. 
% von wie vielen Sätzen sind welche Klassen

% in Tabellen: auf was getestet?

% baseline: random bzw. most-common class

% POS-Tags-threshold, d.h. unseen n-grams -> ungrammatical

% (english resource grammar)

% mapping to source sentences
% Levenshtein auf Wortebene (?)

\maketableofcontents

\section{Introduction}
With an ever-growing amount of information, it is becoming increasingly important to reduce that information to a processible amount, by summarizing it. Most of the time, it is infeasible to do this by hand, and there is thus a need for automating the process.

Automatic summarization is the task of taking one or more documents as input and producing a summary that contains the most important information from these documents.

\textit{Automatic summarization systems} can be divided in \textit{extractive} and \textit{abstractive}, and in \textit{single-document} and \textit{multi-document} summarization systems.

A \textit{single-document} system takes only one document as an input and produces a summary of that document, while a \textit{multi-document} system summarizes multiple documents on the same topic.

An \textit{abstractive} system is one which infers knowledge from its input into an internal semantic representation and produces a novel text based on that knowledge, using language generation techniques. The task of constructing an abstractive summarization system is very complex, which is why most systems used today are \textit{extractive}.

Extractive summarization systems select relevant sentences from the source document(s), reorder them and finally apply sentence fusion and compression to shrink the resulting summary.

We are only concerned with extractive, multi-document summarization, because our data consists of summaries produced by extractive multi-document summarization systems.

Evaluation of automatic summarization systems has largely been done on content coverage. Along with the task being hard in general, this is one reason why generated summaries often lack linguistic quality \citep{nenkova2011foundations}, meaning that they contain many errors of various kinds. A new focus on linguistic quality (LQ) evaluation would presumably help decrease these issues and produce overall better summarization systems.

As a first step in this direction, \cite{friedrichlqvsumm} created the \textit{LQVSumm} corpus, which consists of multi-document summaries from the TAC 2011 shared task on Guided Summarization \citep{owczarzak2011overview}, annotated manually with LQ violations on entity, clause and discourse level.

The corpus also includes annotations on 50 summaries produced by G-Flow \citep{gflow}, which we disregard, because the linguistic quality is much higher than in the rest of the corpus, along with the fact that the size of the annotated G-Flow summaries is much smaller.

Manual annotation is an expensive and time-consuming process, for which an automatic system could be an alternative. Our work deals with creating such a system that manages at least part of the annotation automatically. We develop and evaluate our system on top of the \textit{LQVSumm} corpus. We also limit ourselves to the clause-level violations listed in \cite{friedrichlqvsumm}, except for \textit{no semantic relation}.

The rest of this paper is organized as follows:

The next section discusses related work. In Section~\ref{corpus_study}, we take a look at the \textit{LQVSumm} corpus and the challenges it poses for our experiments and methods. We also demonstrate our subtype system in that section, to prepare for the experiments.

In Section~\ref{experiments} we describe and evaluate methods for annotating ungrammaticality (using decision trees with features extracted from parsing and the high-performing detection of the subtype \textit{missing spaces}), datelines (using regular expressions) and redundancy (using n-gram overlap).

We achieve F-Scores of 58.6\% for ungrammaticality, 87.8\% for datelines and 37.0\% for redundancies. We discuss our results in Section~\ref{discussion}. In Section~\ref{conclusion}, we discuss future work and conclude.

\section{Related Work}
\label{related_work}
% other works on grammaticality and so on

Pioneer work was done by \cite{atwell1987detect}, who used the part-of-speech tagger CLAWS \citep{leech1983automatic} along learned likelihood of POS tag pairs in grammatical sentences to annotate errors using Markov chains.
They then added error tags to their tag dictionary --- tags that similarily spelled words have (when different from the word's own tags) --- and used them to make finding likely typing errors easier.

Most work in the field of automatic grammar checking seems to have been done in the area of second language learning.
\cite{bender2004arboretum} use the LKB parser and generator \cite{copestake2002implementing} with the English Resource Grammar \citep{flickinger2000building} to build \textit{Arboretum}, a system that, when given an erroneous input sentence, detects that error and outputs a corrected version of the sentence. Their method uses manually selected mal-rules \citep{schneider1998recognizing} to detect error types specific to second language learners of English.

While this seems to work for their specific domain, it does not transform well into ours: The mistakes language learners make, and the kind of ungrammaticality summarization systems produce, are very different. Source texts that are being summarized are usually grammatical, and get their ungrammaticality during summarization from being compressed from two formerly grammatical sentences into one ungrammatical one, for example. Mistakes typical for learners of English as a second language either usually do not occur in automatic summarizations (such as misspelt words), or do happen, but due to a different cause (for example agreement).

\cite{chodorow2000unsupervised} build \textit{ALEK}, a system to grade TOEFL essays. It does so by viewing the task as an extension of Word Sense Disambiguation, and collecting n-grams of POS-tags and function words, and then classifying using primitive statistical measures like \textit{Mutual Information} and the $\chi^2$ test. However, only the whole essay is classified into one of six possible scores, and this decision is only made by looking at the test subject's understanding of the three words \textit{Concentrate}, \textit{Interest} and \textit{Knowledge} in context.

\cite{sun2007detecting}, while also citing second language learning as their primary application, use a different approach: Instead of manually writing rules, they employ a supervised statistical learning method. They extract what they call \textit{labeled sequential patterns} (ngrams associated with a class) from both erroneous and correct sentences in their training corpus, enrich them with other automatically computed features, and use them as input for classification models; an SVM \citep[SVMlight,][]{Joachims/02a} and a Na\"ive Bayes classification model. The system achieves F-scores of around 80\%, evaluated on second language learning.

There also has been work in the area of assessing grammaticality of automatically generated summaries, namely \cite{vadlapudi2010automated}. They used n-grams on POS-tag level and evaluated their system by looking at how well their classification correlates to ROGUE scores \citep{lin2004rouge}. \citeauthor{vadlapudi2010automated} achieve a Spearman's $\rho{}$ correlation of up to 77\% to ROGUE scores manually assigned by annotators.

\cite{wagner2007comparative} compare various methods for detecting ungrammaticality:

\begin{description}
	\item[Precision Grammar] After parsing a sentence with the XLE LFG parser \citep{maxwell1996efficient}, a sentence is classified as ungrammatical based on the *-mark (which the parser assigns to any sentence whose optimal solution uses a constraint marked as ungrammatical), parser exceptions during runtime and on whether there is a parse at all.
	\item[POS N-grams] Using 10-fold cross validation, part-of-speech n-grams ($2 \leq n \leq 7$) are counted on the training data, and a sentence is classified as ungrammatical if it contains an n-gram that is below a threshold.
	\item[Decision Trees on XLE Output] Using parser output as features (including whether a sentence is marked with a * (see above), the number of words and the number of parses, amongst others), a J48 decision tree is trained.
	\item[Decision Trees on N-grams] The same experimental setup as in the previous method, but using the frequencies of the rarest n-gram for each possible n ($2 \leq n \leq 7$).
	\item[Decision Trees on Combined Feature Sets] Using the features from the two previous methods.
\end{description}

Their error corpus is an artificial one, generated by introducing systematical errors into otherwise grammatical data. Their best method (\textbf{Decision Trees on Combined Feature Sets}) achieves an F-Score of 66.9\%.

The BLEU score \citep{papineni2002bleu} is a method for evaluating the effectiveness of a machine translation system, which uses a number of (human) reference translations to score an automatic translation. They showed that their score correlates highly with human evaluation, which has lead to the BLEU score having become the standard evaluation metric for machine translation.

\cite{hatzivassiloglou1999detecting} implement a system for redundancy detection on the level of ``text units'' (paragraphs or sentences) using machine learning techniques. As features, they use:

\begin{description}
	\item[Word co-occurence] A very similar feature to our \textbf{Unigrams} approach in Section~\ref{s_unigrams}.
	\item[Matching noun phrases] Using LinkIt \citep{wacholder1998simplex}, they detect simplex noun phrases and match those sharing a head
	\item[WordNet synonyms] Matching words that occur in the same WordNet \citep{fellbaum2005wordnet} synset.
	\item[Common semantic classes for verbs] Matching verbs that share the same semantic class \citep{levin1993english}
	\item[Shared proper nouns] Matching shared proper nouns, identified by Alembic \citep{aberdeen1995mitre}
\end{description}

These features were then optionally normalized by sentence length or relative frequency of the feature occurence.

In addition to these primitive features, they also used three composite features, which combine pairs of primitive features and are applied as restrictions on their primitive features:

\begin{description}
	\item[Order] Whether an element (word) pair had the same relative order in both sentences/textual units, i.e. word X comes before word Y in both textual units, or vice versa, for some words X and Y
	\item[Distance] Whether two elements are no farther than some value in both textual units
	\item[Primitive] Elements are restricted to a specific primitive feature
\end{description}

They then trained a machine learning algorithm \citep{cohen1996learning} and evaluated on a news text corpus, for which they achieved a precision of 60.5\% and a recall of 36.6\%.

\section{LQVSumm}
\label{corpus_study}
% describe the corpus and its flaws
The LQVSumm corpus consists of annotations on summaries produced by systems participating in the TAC 2011 Guided Summarization Task \citep{owczarzak2011overview}.

\begin{table}
\begin{center}
\begin{tabular}{|c|c|c|}
\hline
\textbf{Set} & \textbf{Number of summaries} & \textbf{Topics}\\
\hline
\textit{dev-1} & 351 & D1101 -- D1108\\
\hline
\textit{dev-2} & 352 & D1109 -- D1116\\
\hline
\textit{test} & 1232 & D1117 -- D1144\\
\hline\hline
Overall & 1935 & (D1101 -- D1144)\\
\hline
\end{tabular}
\end{center}
\label{testsets}
\caption{Distribution and count of summaries across development and test sets}
\end{table}

We split the corpus into three parts, while making sure that topics weren't spread across sets: \textit{dev-1} and \textit{dev-2} are development sets, \textit{test} is our test set. We have two development sets, because for some methods, we developed on one part and trained on another.
\textit{dev-1} contains 351 summaries, \textit{dev-2} contains 352, that is 8 topics per development set and about 20\% of the corpus each.
The test set \textit{test} contains the other 60\%, making up 1232 summaries and 28 topics.

The breakdown of the distribution of topics across development and test sets is shown in Table~\ref{testsets}.

The annotation, our experiments and the evaluation are all done on sentence basis. We use Stanford CoreNLP \citep{manning-EtAl:2014:P14-5} for sentence splitting. This is necessary to have classifiable entities, but introduces inaccuracies, because LQVSumm is annotated on clause level, clause meaning an arbitrary substring of a summary, which is often a sentence, but also sometimes spanning over sentence bounds.

\subsection{Violation types}

Some of these annotations were done on the \textit{entity level}, others on the \textit{clause level}. Our work focusses on four clause-level violations.

When a clause is annotated with an \textit{incomplete sentence} violation (INCOMPLSN), the sentence was cut off, most likely due to a summarization system trying to fit as much in the summary as the limit for the task allowed, or due to sentence compression. An example of such a clause would be:

\quad\textit{Neighbors and family members saw no}

Annotations of the violation type \textit{inclusion of datelines} (INCLDATE) mark timestamps that have ended up in the summary. These fragments are the result of the summaries' domain; news articles. Datelines appear in a couple of different variations, most of the time at the beginning of a sentence:

\quad\textit{MADRID, Feb. 15 (Xinhua) Rabei Ousmane Sayed Ahmed,}

This example also shows that multiple violations can of course happen in the same sentence, as it is also tagged as containing an \textit{incomplete sentence} violation.

The violation type \textit{other ungrammatical form} is used for any other ungrammaticality happening in a single phrase. Therefore, we find the biggest variety of violations in here:

\quad\textit{Valley Veterinary Bismarck, North Dakota, a veterinarian, Carlson, has been worried that pet owners to the Menu Foods on its web site.}

\quad\textit{A Boeing 737-400 plane with 102 people on board crashed into amountain in Indonesia's West Sulawesi province on Monday, killingat least 90 people.}

\quad\textit{//www.menufoods.com/recall -- early Saturday.}

The remaining three violation types are done on multiple phrases. A \textit{redundant information} violation (REDUNINF) means two phrases share a lot or all of their information. There are instances of this violation where the two phrases are exactly the same, and others where the information is rephrased in the second sentence:

\quad\textit{Representatives from the commercial fishing industry, environmental groups and government regulators are discussing ways to strengthen information sharing and cooperation among regional organizations to track and manage tuna stocks.}

\quad\textit{Representatives, including the commercial fishing industry, government regulators and environmental groups, plan to discuss ways to strengthen information sharing and cooperation among regional organizations to better manage tuna stock and adopt an action plan, the Japanese Fisheries Agency said in a statement.}

Because the instance counts for the last remaining violation types (\textit{no semantic relatedness} and \textit{no discourse relation}) were so low, they were disregarded and we didn't try to a develop methods to detect them.

\subsection{Subtype system}
\label{s_subtypes}

As a first step, we inspect the corpus. We find that the type system of the LQVSumm corpus \citep{friedrichlqvsumm} is not detailed enough for our purposes and has a few shortcomings: Especially under the label of \textit{other ungrammatical form} many different types of errors are combined.

Since we assume that dividing the problem into smaller, easier problems could help us detect \textit{other ungrammatical form} violations, we decide to define a number of subtypes for this violation:

\begin{description}
\item[missing spaces] \hfill \\
	This is the most common type of error: The sentence contains a word that does not exist. In almost all cases, this happens when whitespace between two words is missing.

	Example: \textit{A strong earthquake measuring 7.8 magnitude struck \\\textbf{Wenchuancounty} of Sichuan Province on Monday, leaving at least \\\textbf{12,000people} died and thousands more injured.}

\item[missing words] \hfill \\
	One or multiple words are clearly missing. These seem to most often be function words such as articles or pronouns, rather than content words. In the example, an underscore marks the position where a word, probably ``was'' is missing.

	Example: \textit{An Israeli woman \_ killed and 11 others were wounded in the suicide bombing at a shopping mall in southern Israeli town of Dimona.}

\item[punctuation error] \hfill \\
	Most of the time, this means: Punctuation is missing, but it can also mean there is something else wrong with punctuation in that sentence which makes it ungrammatical, as can be seen in the example: Unbalanced parantheses.

	Example: \textit{China has allocated 200 million yuan (million dollars for disaster relief work after an earthquake rocked the country's killing at least seven people, state reported on Tuesday.}

\item[capitalization error] \hfill \\
	A word that should be capitalized is not or one that should not be capitalized is.

	Example: \textit{earlier on \textbf{m}onday \textbf{g}erman chancellor \textbf{a}ngela \textbf{m}erkel and foreign \\minister \textbf{f}rank \textbf{w}alter \textbf{s}teinmeier offered their condolences to \textbf{c}hina over the heavy loss of life in the powerful earthquake that hit \textbf{c}hina's southwestern province of \textbf{s}ichuan.}

\item[unparsable]\hfill \\
	This subtype means that the (human) annotator was unable to make sense of the sentence and considers it not correctly parsable. Most instances of this subtype seem to happen when sentence fusion occurs; the summarization system combined two or more sentences in an ungrammatical or meaningless way.
	%This subtype looks similar to a punctuation error, but differs in that sentences are intermixed with each other; in the middle of one sentence, the reader suddenly finds themself in a different one. This could also happen if part of a sentence was removed. In the example, an underscore marks the point at which the break happens.

	Example: \textit{All of those provinces and Chongqing, a special municipality \_ deepest condolences to those who lost their loved ones in the devastating natural disaster.}

\item[heading] \hfill \\
	The sentence contain (usually at the beginning) a sequence of capitalized or otherwise heading-like words that aren't part of a dateline.

	Example: \textit{THE CURRENT FIX: Internet applications such as firewalls and spam filters attempt to control security threats.}

\item[incomplete sentence] \hfill \\
	The type system of the LQVSumm corpus \citep{valeeva} defines an \textit{incomplete sentence} violation as words being cut off at the end of a sentence. A couple of times though, this also occurs at the beginning of a sentence. We restructure the type system by treating all types of incomplete sentences as this subtype of \textit{other ungrammatical form}.

	Example: \textit{A Palestinian suicide bomber detonated an explosive belt at a \\commercial center in Dimona on}.
\item[should be other type] \hfill \\
	Sometimes sentences that are tagged as \textit{other ungrammaticality} should (also) be tagged as another type, such as sentences containing datelines. This doesn't necessarily mean that the sentence contains no other ungrammaticalities. This subtype be caused by inconsistencies in the corpus, or from the process of mapping the annotated violations to sentences. While the following example is an \textit{incomplete sentence}, it also contains a dateline.

	Example: \textit{its deepest sympathy to the Israeli people,\textbf{00 a.m. local time (0800 GMT)} as a suicide bomber detonated an explosive belt in the commercial center, according to local media reports.}
%\item[not ungrammatical] \hfill \\
%	As is to be expected, the annotation of the corpus isn't correct 100\% of the time. This subtype merely denotes a correct sentence that was incorrectly marked as ungrammatical.
%
%	Example: \textit{One Israeli woman was killed and at least eight others wounded on Monday in a suicide bombing which ripped through a commercial center in the southern Israeli town of Dimona, the first attack of the kind since January 2007.}

%%% did not occur in dev-2

\end{description}

After manually counting and investigating these subtypes on \textit{dev-1} we then mark these types in \textit{dev-2} for every sentence that is tagged as \textit{other ungrammatical form} in the corpus. We allow sentences to have multiple subtypes and also annotate \textit{incomplete sentence} violations with additional (sub)types, if applicable. The frequency distribution of the subtypes in \textit{dev-2} can be seen in Figure~\ref{subtypes}.

It is to be noted, that from here on, we consider \textit{incomplete sentence} to be a subtype of \textit{other ungrammatical form}.

\begin{figure}
\begin{center}
\includegraphics[scale=0.6]{subtypes_dev2.png}
\end{center}
\caption{\textbf{Subtypes of ungrammaticality in \textit{dev-2}.} Where the annotator didn't know what subtype to annotate the sentence with, they selected ``other''.}
\label{subtypes}
\end{figure}

\section{Experiments}
\label{experiments}
% All experiments, intermixed with small discussion segments
%TODO more intro
We conduct a number of experiments to find reliable ways to detect different kinds of linguistic quality violations. If possible, we would like to eventually be able to reproduce the annotations of our types on the LQVSumm corpus as well as annotate violations in other corpora.

\begin{figure}
	\begin{subequations*}
	\begin{align*}
		\text{Precision} &= \frac{\text{true positives}}{\text{true positives} + \text{false positives}}\\
		\text{Recall} &= \frac{\text{true positives}}{\text{true positives} + \text{false negatives}}\\
		\text{F-Score} &= 2*\frac{\text{precision}*\text{recall}}{\text{precision}+\text{recall}}\\
		\text{Accuracy} &= \frac{\text{true positives}+\text{true negatives}}{\text{all instances}}
	\end{align*}
	\end{subequations*}
	\label{metrics}
	\caption{Standard metrics Precision, Recall, F-Score and Accuracy}
\end{figure}

We use the standard metrics of Precision, Recall, F-Score, and Accuracy, which can be seen in Figure~\ref{metrics}.

\subsection{Detecting Ungrammaticality}
\label{sec_unknowntokens}
We start with the violation type \textit{other ungrammatical type} (plus \textit{incomplete sentence}, as explained earlier).

Perhaps the most noticeable result of our corpus study is the large portion of \textit{missing spaces} violations, meaning clauses that include tokens which aren't correct words. Almost all of these cases come from a missing space between two words forming tokens such as \textit{reportsreaching} or \textit{Wenchuancounty}. As there is such a large amount of this type, finding a reliable detection method for this subtype would presumably significantly boost detection of ungrammaticality in general.

\subsubsection{The UnknownTokens method}
We call our straightforward approach to identifying missing spaces \textbf{UnknownTokens}: Tagging a sentence as containing a \textit{missing spaces} ungrammaticality if and only if there is a token in the sentence that isn't a ``known'' token, i.e. one that doesn't exist in a list of known words. In our setting, we can easily deduce such a list from the source documents of the automatic summarization process. If the source document is available and used as a base for our vocabulary, we call that approach \textbf{UnknownTokens-source}.

In case the original documents aren't available or this method should be used in a different setting, we also investigate how to adapt our approach to that. This approach has a few steps, which we will all evaluate individually:
First, an initial list of words is created from a large text corpus (\textbf{UnknownTokens-gw}); we use the first 20\% of the AFE part of the gigaword corpus \citep{gigaword}. If the corpus is large and general enough, this will probably cover a large vocabulary.

Because this will not include words for named entities yet, we only consider a token to be an unknown token, if it is not tagged as a Named Entity by a Named Entity Recognizer (\textbf{UnknownTokens-ner}). We employ the Stanford Named Entity Recognizer \citep{stanfordNER}, which is a widely used, state-of-the-art NER that comes with a model for the English language.

In order to improve upon the NER and further increase recall, we use two simple heuristics (\textbf{UnknownTokens-heur}): Firstly, we assume that words that start with a capital letter and are only followed by lower case letters are named entities and therefore no unknown tokens and we throw out words containing hypens. Finally, we automatically check whether an unknown token has an entry on the online encyclopedia Wikipedia (\textbf{UnknownTokens-wiki}).

We automatically label any tokens that are neither on our list of known tokens from GigaWord, nor tagged as a named entity as ``unknown'' and tag the sentence as containing unknown tokens. The combined method \textbf{UnknownTokens-gw+heur+ner+wiki} is what we call \textbf{UnknownTokens-general}.

\subsubsection{Evaluation of UnknownTokens}
We evaluate how effective \textbf{UnknownTokens} is in detecting \textit{missing spaces} violations, both with (\textbf{UnknownTokens-source}) and without (\textbf{UnknownTokens-general}) access to the source documents, and compare it to a \textbf{baseline}, which always assumes the most common case (no \textit{missing spaces} violation).
The standard metrics precision, recall and F-score are used for evaluation.

Table~\ref{eval_unknowntokens} shows the results of the evaluation of both variations of \textbf{UnknownTokens} on \textit{dev-2}. Because the amount of sentences not containing a missing spaces violation is much larger than the ones that do, several weighted metrics are provided, too:

The micro-average method consists of summing up the individual values from the confusion matrix (True Positive, False Positive, True Negative and False Negative) and calculating new metrics based on the results. We only display the F-score here, since F-score, recall and precision are identical for a 2-class scenario: Since the true positives of the missing spaces class are identical with the true negatives of the no missing spaces class (and vice versa), the new number of true positives and true negatives (for the micro average) is $tp + tn$ of any of the two classes. The same goes for false negatives and false positives. That means that when only dealing with two classes, in the micro-average method, the terms \textit{positive} and \textit{negative} can be used interchangeably. As precision is defined as $\frac{tp}{tp+fp}$ and recall as $\frac{tp}{tp+fn}$; as the equations only differ in whether they use $fp$ or $fn$ (which are exactly the same here) in the divisor, precision and recall (and therefore F-score, too, since the F-score is simply the harmonic mean of precision and recall) are identical.

Macro-averages are just arithmetic means of precision and recall. For both micro- and macro-average, the new F-score is calculated from the new scores.

Both \textbf{UnknownTokens-general} and \textbf{UnknownTokens-source} improve vastly over the baseline, but it is clear, that if the source text is available, it should be used to infer known words.

%%%BEGIN BACKUP WITHOUT WEIGHTED MEAN:
\begin{sidewaystable}
\begin{center}
\begin{tabular}{r|c|c|c|c|c|c|c|c|}
& \multicolumn{3}{c|}{Missing spaces} & \multicolumn{3}{c|}{No missing spaces} & Macro-average & Accuracy\\
& \textbf{P} & \textbf{R} & \textbf{F} & \textbf{P} & \textbf{R} & \textbf{F} & \textbf{F} & \\
\hline
\textbf{Baseline} & 0.0 & 0.0 & 0.0 & 94.8 & \textbf{100} & 97.3 & 48.7 & 94.8\\
\hline
\textbf{UT-gw} & 15.0 & \textbf{98.7} & 26.0 & \textbf{99.9} & 69.1 & 81.7 & 68.2 & 70.7\\
\hline
\textbf{UTgw+heur} & 30.5 & 97.3 & 46.5 & 99.8 & 87.8 & 93.4 & 76.5 & 88.3\\
\hline
\textbf{UT-gw+heur+ner} & 35.5 & 97.3 & 52.0 & 99.8 & 90.3 & 94.8 & 78.6 & 90.6\\
\hline
\textbf{UT-gw+heur+ner+wiki} & 70.3 & 96.0 & 81.2 & 99.8 & 97.8 & 98.8 & 90.6 & 97.7\\
\hline
\textbf{UT-source} & \textbf{95.9} & 94.6 & \textbf{95.2} & 99.7 & 99.7 & \textbf{99.7} & \textbf{97.5} & \textbf{99.5}\\
\end{tabular}
\end{center}
\caption{Evaluation of \textbf{UnknownTokens} on \textit{dev-2}, in percent. \textbf{UT-wordnet+heur+ner+wiki} is equivalent to \textbf{UnknownTokens-general}. \textbf{wn} = Wordnet-based dictionary, \textbf{heur} = title case heuristics, \textbf{ner} = Stanford NER, \textbf{wiki} = Wikipedia entry check}
\label{eval_unknowntokens}
\end{sidewaystable}
%%%END BACKUP WITHOUT WEIGHTED MEAN

%%%BACKUP WITH WEIGHTED MEAN:
% \begin{sidewaystable}
% \begin{center}
% \begin{tabular}{r|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|}
% & \multicolumn{3}{c|}{Positive Set} & \multicolumn{3}{c|}{Negative Set} & \multicolumn{3}{c|}{Weighted Mean} & Micro-average & \multicolumn{3}{c|}{Macro-average}\\
% & \textbf{P} & \textbf{R} & \textbf{F} & \textbf{P} & \textbf{R} & \textbf{F} & \textbf{P} & \textbf{R} & \textbf{F} & \textbf{F} & \textbf{P} & \textbf{R} & \textbf{F}\\
% \hline
% \textbf{Baseline} & 0.0 & 0.0 & 0.0 & 94.79 & 100 & 97.33 & 89.85 & 94.79 & 92.26 & 94.79 & 47.40 & 50.0 & 48.67\\
% \hline
% \textbf{UT\textsubscript{wn}} & 14.96 & 98.65 & 25.98 & 99.89 & 69.14 & 81.72 & 95.47 & 70.68 & 78.82 & 70.68 & 57.43 & 83.90 & 68.19\\
% \hline
% \textbf{UT\textsubscript{wn+heur}} & 30.51 & 97.30 & 46.45 & 99.83 & 87.81 & 93.43 & 96.22 & 88.30 & 90.98 & 88.30 & 65.17 & 92.56 & 76.49\\
% \hline
% \textbf{UT\textsubscript{wn+heur+ner}} & 35.47 & 97.30 & 51.99 & 99.84 & 90.26 & 94.81 & 96.49 & 90.63 & 92.58 & 90.63 & 67.66 & 93.78 & 78.61\\
% \hline
% \textbf{UT\textsubscript{wn+heur+ner+wiki}} & 70.30 & 95.95 & 81.15 & 99.77 & 97.77 & 98.76 & 98.23 & 97.68 & 97.84 & 97.67 & 85.04 & 96.86 & 90.57\\
% \hline
% \textbf{UT\textsubscript{source}} & 95.89 & 94.59 & 95.24 & 99.70 & 99.70 & 99.70 & 99.50 & 99.43 & 99.47 & 99.50 & 97.80 & 97.15 & 97.47\\


% \end{tabular}
% \end{center}
% \caption{Evaluation of \textbf{UnknownTokens} on \textit{dev-2}, in percent. \textbf{UT\textsubscript{wordnet+heur+ner+wiki}} is equivalent to \textbf{UnknownTokens\textsubscript{general}}. \textbf{\textsubscript{wn}} = Wordnet-based dictionary, \textbf{\textsubscript{heur}} = title case heuristics, \textbf{\textsubscript{ner}} = Stanford NER, \textbf{\textsubscript{wiki}} = Wikipedia entry check}
% \label{eval_unknowntokens}
% \end{sidewaystable}
%%%END BACKUP WITH WEIGHTED MEAN


\subsubsection{The RandomForest-UT method}
To detect more general forms of ungrammaticality, several other methods were investigated. Following \cite{wagner2007comparative}, we learn decision trees from outputs of several systems that on their own have not proven sufficient for this task. For this experiment, the development set \textit{dev-1} was used.

We train a language model on parts (the same parts we used as a word source in \textbf{UnknownTokens-general} (section~\ref{sec_unknowntokens})) of the Gigaword corpus\citep{gigaword}, extract perplexity scores for the sentences in our test set, and annotate them automatically. For the language model generation and the extraction of perplexity scores, we employ SRILM~\citep{stolcke2002srilm}, a language modeling toolkit. The perplexity scores alone had shown not to be effective in detecting ungrammaticality.


%The free WEKA software~\citep{hall2009weka} is an allround machine learning toolkit.
We then use a RandomForest \citep{breiman2001random} to learn decision trees.
RandomForest works by randomly selecting subsets of training instances along with random subsets of features to train a number of decision trees and then classifying test instances with what the majority of trees are classifying the instance with.

For this task, we use the machine learning toolkit WEKA \citep{hall2009weka}.
%As an input, WEKA requires an ARFF file, which we created using three features:
We use three features for our decision tree:

\begin{itemize}
	\item \textbf{LM\_PP} is the perplexity calculated for a sentence by the language model, following \cite{sun2007detecting}. This is a numeric feature. It proved to not be sufficient for detecting ungrammaticality on its own.
	\item \textbf{missing spaces} is the classification of \textbf{UnknownTokens-source} for a given sentence, as described in section~\ref{sec_unknowntokens}. This is a binary feature.
	\item \textbf{numberofwords} is the number of words in the sentence. We follow \cite{wagner2007comparative} here, the intuition is that ungrammatical sentences will often be extraordinarily long, or very short:

		\textit{Former US President George W Bush, Who Will attend a memorial service on Tuesday, Virginia, and the university of at least 33 people Were killed, in the Worst shooting rampage in modern history, the White House said.}

		\textit{The report of a}
\end{itemize}

% \subsubsection{Evaluation of RandomForest-UT}
% \label{S_eval_randomforest}

% We performed 10-fold cross-validation, which lead to a weighted F-score of 86.3\%. A total of 1215 (88.1\%) instances was classified correctly, 164 (11.9\%) incorrectly.

% === Run information ===

% Scheme:       weka.classifiers.trees.RandomForest -I 10 -K 0 -S 1 -num-slots 1
% Relation:     ungrammaticality
% Instances:    1379
% Attributes:   4
%               language_model
%               nonwords
%               numberofwords
%               class
% Test mode:    10-fold cross-validation

% === Classifier model (full training set) ===

% Random forest of 10 trees, each constructed while considering 3 random features.
% Out of bag error: 0.1291



% Time taken to build model: 0.43 seconds

% === Stratified cross-validation ===
% === Summary ===

% Correctly Classified Instances        1215               88.1073 %
% Incorrectly Classified Instances       164               11.8927 %
% Kappa statistic                          0.3351
% Mean absolute error                      0.1491
% Root mean squared error                  0.3094
% Relative absolute error                 65.2325 %
% Root relative squared error             91.6119 %
% Coverage of cases (0.95 level)          96.2292 %
% Mean rel. region size (0.95 level)      66.4612 %
% Total Number of Instances             1379     

% === Detailed Accuracy By Class ===

%                  TP Rate  FP Rate  Precision  Recall   F-Measure  MCC      ROC Area  PRC Area  Class
%                  0.970    0.707    0.901      0.970    0.934      0.361    0.766     0.938     ungrammaticality-no
%                  0.293    0.030    0.596      0.293    0.393      0.361    0.766     0.417     ungrammaticality-yes
% Weighted Avg.    0.881    0.618    0.861      0.881    0.863      0.361    0.766     0.869     

% === Confusion Matrix ===

%     a    b   <-- classified as
%  1162   36 |    a = ungrammaticality-no
%   128   53 |    b = ungrammaticality-yes

\subsubsection{The RandomForest-UT+parser method}
We also investigated whether parser output would improve the results. For this task, we utilized the HPSG parser Answer Constraint Engine \citep{ace} with the English Resource Grammar \citep{copestake2002implementing} to parse the entire corpus and annotate it with output statistics of the parser: Following \cite{wagner2007comparative} loosely, we extracted the number of readings/parses, the amount of memory the parser used for the parse and the success status, which could be either ``ok'' or ``ignored'', the latter appearing when the parsing failed entirely for some reason.

We then added these three features to \textbf{RandomForest-UT} and call the new method \textbf{RandomForest-UT+parser}.

\subsubsection{Evaluation of RandomForest}

\begin{table}
\begin{center}
\begin{tabular}{r|c|c|c|c|}
  & \textbf{Precision} & \textbf{Recall} & \textbf{F-Score} & \textbf{Accuracy}\\
  \hline
  Baseline & - & 0 & - & 77.7\\
  \hline
  RandomForest-UT & 65.5 & 44.2 & 52.8 & 82.3\\
  \hline
  RandomForest-UT+parser & \textbf{72.8} & \textbf{49.1} & \textbf{58.6} & \textbf{84.5}\\
  \hline
  \end{tabular}
\end{center}
\caption{Evaluation of \textbf{RandomForest-UT} and \textbf{RandomForest-UT+parser} on \textit{test}, in percent, based on the class of ungrammatical sentences. As Baseline, we classified every sentence with the most-frequent class, ``not ungrammatical''.}
\label{eval_randomforest}
\end{table}


We performed 10-fold cross-validation with both \textbf{RandomForest-UT} and \textbf{RandomForest-UT+parser} on \textit{test}, the results of which can be seen in Table~\ref{eval_randomforest}. These evaluations are done on the class of ungrammatical sentences, evaluating it on both classes leads to much higher scores (with a weighted F-score of 83.4\% for both ``ungrammatical'' and ``not ungrammatical'').

As evident from the table, including the parser output statistics as features (\textbf{RandomForest-UT+parser}) significantly improves upon the performance of just \textbf{RandomForest-UT}. We generally value precision more than recall, because we prefer it if our system misses a case of ungrammaticality to it falsely classifying a grammatical sentence as ungrammatical.

An ablation study can be seen in Table~\ref{ablation_study_randomforest}. No number being very high indicates that we have some redundancy of information in the features here.
\begin{table}
\begin{center}
\begin{tabular}{c|c}
  \hline\hline
  Feature & Decrease in F-Score\\%84.5391%0.586
  \hline
  Parser Readings & 5.7\\%478\\0.586-0.529
  Number of Words & 2.2\\%34\\0.586-0.564
  Language Model Perplexity & 1.4\\%1.2091\\0.586-0.572
  Parser RAM & 1.3\\%01\\0.586-0.573
  UnknownTokens Output & 0.8\\%0.3369\\0.586-0.578
  Parser Status & 0.4\\%1982\\0.586-0.582
  \hline\hline
  \end{tabular}
  \end{center}

  \caption{Ablation study of \textbf{RandomForest-UT+parser}. The numbers denote the decrease of the F-Score if that feature were to be taken out.}
  \label{ablation_study_randomforest}
 \end{table}

When evaluating the emerging decision tree on general ungrammaticality using the same method as in Section~\ref{S_eval_randomforest}, it yielded worse results than \textbf{RandomForest-UT} alone. We therefore inspected whether it could instead be used to find instances of the subtype \textit{unparsable}. This lead to a precision of 86.6\%, a recall of 87.9\%, and an F-Score of 86.3\%. A total of 87.9\% of the instances was classified correctly.

\subsection{Datelines}
\label{s_datelines}
Source texts from news articles, like the ones in the TAC dataset, often contain datelines, which sometimes make it into the generated summary. While in the context of the article datelines made sense (often displayed smaller or otherwise visually distinctive), they are not needed, distracting and confusing in the summaries.

Datelines take many different forms, with some including a location. There is also a variety of appearing time formats:

\quad\textit{\textbf{BLACKSBURG, Virginia 2007-04-16 18:34: 44 UTC} A gunman opened fire in a dorm and classroom at Virginia Tech on Monday, killing at least 30 people in the deadliest shooting rampage in U.S. history.}

\quad\textit{\textbf{BERLIN, May 13( Xinhua)} The German government announced on Tuesday that it is to provide 500, 000 euros( around 770, 000 U.S. dollars) in aid for earthquake victims in Sichuan Province of China.}

\quad\textit{\textbf{00 a.m.}People are panicking.}

\subsubsection{Method}
The emerging patterns are so regular, that we use regular expressions to detect them. A few iterations of the expression were created, but so far the highest results were achieved using the regular expression

\begin{lstlisting}
UTC|^\d{4}-\d{2}-\d{2}|^[A-Z]{3,}|^(Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)
\end{lstlisting}

The expression has four parts, of which only one has to match to make the system classify a sentence as containing datelines: The first part matches the string ``UTC'' (Coordinated Universal Time, a common fragment in datelines), the second one numeric dates like \textit{2014-04-13}, the third one capitalized words that are longer than 4 letters and are therefore expected not to be abbreviations (an example would be \textit{LONDON, Feb. 4 (Xinhua)}), and the final part matches abbreviated names of months. Only ``UTC'' can occur anywhere, everything else needs to occur at the beginning of a sentence to be counted.

\subsubsection{Evaluation}
This very simple approach was evaluated on \textit{test} and leads to good results that can be seen in Table~\ref{eval_regex}. As a baseline, we again classified all sentences with the most-frequent class: not containing a dateline violation.

\begin{table}
\begin{center}
\begin{tabular}{r|c|c|c|c|}
  & \textbf{Precision} & \textbf{Recall} & \textbf{F-Score} & \textbf{Accuracy}\\
  \hline
  Baseline & - & 0 & - & 89.5\\
  \hline
  Regex & 86.0 & 89.7 & 87.8 & 97.4\\
  \hline
  \end{tabular}
\end{center}
\caption{Evaluation of \textbf{Regex} on \textit{test}.}
\label{eval_regex}
\end{table}

%dev2: 90.7\% (precision: 90.3\%, recall: 91.1\%)

When looking into the inaccurately detected instances, the inconsistencies of the LQVSumm corpus make up most of the misses:

\textit{IVORY TUSKS}

is classified as containing a dateline by the regular expression, but is not annotated as such in the corpus. An example of a false negative would be:

\textit{00 a.m. local time (0800 GMT)}

which our system doesn't match.

\subsection{Redundancy}
\label{redundancy}

Automatic summaries constructed from multiple source texts on the same topic often suffer from redundancy, as sentences that contain the same or largely the same information are occasionally included in the summary. As a consequence, reading fluency is impaired.

While --- in contrast to many other linguistic quality violations --- this also has an impact on traditional evaluation scores for automatic summarization systems (because the content coverage will be lower than it could be), it is also considered to be a problem of our domain.

In the LQVSumm corpus, different degrees of redundant sentence pairs occur. Some sentence pairs annotated as redundant contain similar information, but using completely different phrasing and focus:

%low example
\textit{According to a survey by the State Food and Drug Administration, 65 percent of the respondents worried about the food safety situation in China.}

\textit{Food and drug safety has become a major concern of Chinese people.}

Other pairs of sentences are apparently at least partly composed of the same source sentence, but are still different. One of the sentences might contain additional information and the two sentences might still be of very different length:

%medium example
\textit{Cyclone Sidr, described as the worst storm in years to hit low-lying and disaster-prone Bangladesh, crashed into the southwestern coast Thursday night before sweeping north over the capital Dhaka.}

\textit{The cyclone hit the southwestern coast of Bangladesh on Thursday before sweeping north to the capital Dhaka.}

Finally, sometimes subsequent sentences have marginal differences:
%high example
\textit{An unknown number of cats and dogs suffered kidney failure and about 10 died after eating the affected pet food, Menu Foods said in announcing the North American recall.}

\textit{An unknown number of cats and dogs suffered kidney failure, and about 10 died after eating the affected pet food, Menu Foods said in announcing the North American recall.}

We are seeking to find all of these sentences, but concentrate on the latter two cases where the redundancy is obvious for humans.

\subsubsection{Method}
\label{s_unigrams}
%bigrams stemmed vs. unstemmed in table? No, no sign. diff.

Our method (\textbf{Unigrams}) consists of a simple unigram overlap score, which we found to outperform more sophisticated approaches:

First, we remove all non-alphanumeric characters from both sentences and split them into words. We then treat the words as a set and divide the cardinality of the intersection of each sentence's set by the number of possible overlaps, which will be the number of unique tokens in the smaller sentence. A given threshold is then used to classify instances.

A number of other methods are also implemented: \textbf{Bigrams} is a variation of the \textbf{Unigrams} method which uses bigrams instead of unigrams. \textbf{Combined} is using a score that the mean of the \textbf{Unigrams} and \textbf{Bigrams} scores.

\textbf{Levenshtein} consists of the standard edit distance\citep{levenshtein1966binary} on word level. A threshold of 2 yielded best results on our development set, meaning that sentences are classified as containing redundancies if their word-level Levenshtein distance is 1 or 0. Prior to computation of the score, non-alphanumeric characters were again pruned from the sentences.

Finally, as a further \textbf{Baseline} we classify all sentence pairs as containing redundancies.

We briefly considered using BLEU scores \citep{papineni2002bleu} for redundancy detection (with the rationale that if the second sentence is a ``good translation'' of the first one, it might be redundant), but BLEU scores get their performance from having multiple reference translations available, which is not the case here.

\subsubsection{Evaluation}

In the corpus, redundancy isn't annotated on sentence base, but rather on clause level, where ``clause'' is an arbitrary substring of the summary text. That means that redundancy annotations can span over many sentences, can skip sentences inbetween and can be contained in a single sentence.

For our purposes, we only consider redundancies that occur in two subsequent sentences and our evaluation metrics are based on those.

To obtain a good threshold for \textbf{Unigrams}, \textbf{Bigrams}, and \textbf{Combined}, we compare their performance on the development sets \textit{dev-1} and \textit{dev-2}. The results can be seen in Figure~\ref{redundancy_graph}.

\begin{figure}
\begin{center}
\includegraphics[scale=0.15]{a.png}
\end{center}
\caption{Performance of various thresholds for \textbf{Unigrams}, \textbf{Bigrams}, and \textbf{Combined} in \textit{dev-1} and \textit{dev-2}. Continuous lines are recall, dashed lines are precision and bold lines are F-score.}
\label{redundancy_graph}
\end{figure}

For recall, \textbf{Unigrams} outperforms \textbf{Bigrams}, while for precision \textbf{Bigrams} is predominant.

As is to be expected, higher thresholds correlate with higher precision and lower recall. And while the F-scores are given in Figure~\ref{redundancy_graph} for informational purposes, they are not what we want to optimize; while recall is desirable, we feel that precision is much more important for a system that annotates violations.

As can be seen in Table~\ref{thresholds}, we therefore choose a threshold of $0.4$ for the \textbf{Bigrams} and \textbf{Combined}, and one of $0.5$ for \textbf{Unigrams}, because there appear to be local maxima at that point with still some significant amount of recall.

The results can be seen in Table~\ref{eval_redundancy}. Evidently, the \textbf{Unigrams} method outperforms \textbf{Levenshtein} and the \textbf{Baseline} as well as the other n-gram methods.

We unsuccessfully tried to increase our performance by a few means:

\begin{itemize}
\item stemming the words with the Porter Stemming Algorithm \citep{porter1980algorithm} from NLTK \citep{bird2009natural}
\item part-of-speech-tagging the tokens with NLTK and only counting overlaps that include a verb (with the motivation of ruling out noun-based false positives)
\item looking up the first synset of each token in WordNet \citep{fellbaum2005wordnet}, and building the intersections between the sets by considering two tokens to be the same if the first word is included in the second one's synset.
\end{itemize}

Neither of these methods had any positive effect on our results.

\begin{table}
\begin{center}
\begin{tabular}{|c|c|c|}
\textbf{Unigrams} & \textbf{Bigrams} & \textbf{Combined} \\
\hline
0.5 & 0.4 & 0.4
\end{tabular}
\end{center}
\caption{Thresholds for \textbf{Unigrams}, \textbf{Bigrams}, and \textbf{Combined} determined on \textit{dev-1+2}}
\label{thresholds}
\end{table}

\begin{table}
\begin{center}
\begin{tabular}{r|c|c|c|}
& Precision & Recall & F-Score \\
\hline
\textbf{Baseline} & 4.5\% & \textbf{100\%} & 8.7\% \\
\hline
\textbf{Levenshtein} & 15.8\% & 17.3\% & 3.1\% \\
\hline
\textbf{Unigrams} & \textbf{58.0\%} & 28.2\% & \textbf{37.0\%} \\
\hline
\textbf{Bigrams} & 55.6\% & 14.5\% & 22.9\% \\
\hline
\textbf{Combined} & 56.8\% & 24.3\% & 34.0\% \\
\hline
\end{tabular}
\end{center}
\caption{Evaluation of \textbf{Unigrams}, \textbf{Bigrams}, and \textbf{Combined} on everything but \textit{dev-1} and \textit{dev-2}}
\label{eval_redundancy}
\end{table}

% Bigrams: 0.5555555555555556,0.14450867052023122,0.22935779816513763
% Combined: 0.5675675675675675,0.24277456647398843,0.340080971659919
% Unigrams: 0.5802469135802469,0.27167630057803466,0.3700787401574803

% Levenshtein: 0.15789473684210526,0.017341040462427744,0.03125

% Baseline: 0.045240585774058575,1.0,0.08656492369276958

%\subsection{Unrelatedness} %evtl %NOOO
%\section{Evaluation} oder auch nicht
% or skip entirely and put in experiments? final results here?

\section{Discussion}
\label{discussion}
% What seem to be the best methods for the various tasks and why?
Detecting linguistic quality violations automatically is not a trivial task. It is not always easy for humans to produce consistent annotations and annotations aren't necessarily highly consistent across humans, as can be seen in the inter-annotator agreement in \cite{friedrichlqvsumm}.

Despite that, sometimes the easier methods outperformed the more sophisticated approaches.

In theory, \textbf{UnknownTokens-source} should return perfect results, with 100\% recall, precision and F-score. So why doesn't it? Some of this can probably be attributed to annotation mistakes during the annotation process of the subtypes. Manual inspection of the false positives showed that as the datelines (Section~\ref{s_datelines}) are not a part of the text of the source document (which we took the tokens from), geographical names only present in the datelines were wrongly assumed to be unknown tokens.

\textbf{UnknownTokens-general}'s precision is weakened by a few words such as ``predisposes'' that were neither found in our Gigaword tokens, nor detected to be a named entity by either of our methods. The reader should keep in mind that because we are only evaluating this on \textit{dev-2} (as the subtypes are only annotated there) a couple of words can make a difference.

Without finding evidence for this in our results, in theory cases could occur where a token is a known token (and rightly so), but can also be formed from two other tokens, when missing a space. This could not be correctly classified without contextual knowledge. A fictitious example could be:

\quad\textit{To try this taskmaster the other one first.}

As taskmaster is an English word, it might occur in the list of known tokens, but will still be annotated as missing a space, because \textit{in this context} it is a contraction of \textit{task} and \textit{master}.

This means that achieving 100\% precision with this method is theoretically impossible.

We believe our method of detecting datelines is close to the optimum achievable with a regular expression based approach without accidentally detecting times, time ranges or locations that are mentioned in the text as datelines.
When looking into the false positives, our decision to match abbreviations of month names leads to some sentences being classified as containing a dateline which actually do not contain one:

\quad\textit{\textbf{Mar}uca-Kovac said he phoned her at about 1 p.m. Wednesday, telling her he had left}

Other false positives could have been annotated as containing datelines in the corpus, but weren't:

\quad\textit{\textbf{Nov}ember 17 itself fired a rocket against the US embassy in 1996}

Our biggest loss in terms of false negatives is that we don't detect datelines of the form:

\quad\textit{14( Xinhua)}

Out of all of our best methods, our \textbf{Unigrams} method for detection of redundancies achieves the lowest scores. Part of that can be attributed to the simplicity of our approach: A token based method can only get so far when a subsequent sentence contains paraphrased content from the previous sentence.

\quad\textit{\textbf{Defense Secretary Robert Gates} named an independent review panel \textbf{Friday} to investigate what he called an unacceptable situation'' in outpatient care at \textbf{Walter Reed Army Medical Center}, \textbf{and he} said that some soldiers most directly involved'' in the problems have been removed from their positions.}

\quad\textit{\textbf{Defense Secretary Robert} M. \textbf{Gates} said \textbf{Friday} that he would move swiftly to improve conditions for wounded soldiers being treated as outpatients at \textbf{Walter Reed Army Medical Center}, \textbf{and he} warned that senior officials would be held accountable for the poor conditions.}

While this example has some token overlap, the paraphrased content fails to be noticed by our system.

Another cause of a relatively low score is that in the LQVSumm corpus, redundancy annotations can be done on a smaller scale than what is classified with using our method. For example, a repeated phrase within two long sentences could be marked in the corpus, but our or similar approaches will either not find these cases, or classify too much.

The LQVSumm corpus that we used for all of our experiments is, unfortunately, not large enough to have a wide coverage of all violations. This can be seen most prominently when trying to detect redundancy (Section~\ref{redundancy}): While the corpus itself contains 504 \textit{redundant information} violations \citep[see][]{friedrichlqvsumm}, only 173 of them could be used by us, as we needed redundancies that occured in subsequent sentences and only within the bounds of those.
However, there's little to no alternatives available.

Apart from its size, the corpus is also sometimes annotated inconsistently or at least challenging for our purposes. In general, annotations are done on the clause level, where clauses are arbitrary substrings of the summary. As we classify and evaluate on sentences produced by the sentence splitter of Stanford CoreNLP \citep{manning-EtAl:2014:P14-5}, this introduces noise. For our purposes, a corpus that had been segmented before annotation would be preferable.

As can be seen in Section~\ref{s_subtypes}, the instances of \textit{other ungrammatical form} could have been divided into several types, which would help the automatic classification.

%show example
A legitimate criticism of our work is that our methods, experiments, and results are tailored to the LQVSumm corpus and to the summarization systems that participated in the TAC 2011 Guided Summarization task \citep{owczarzak2011overview}.


\section{Conclusion \& Future Work}
\label{conclusion}

In this paper, we have presented a method to detect the kind of ungrammaticality that occurs in automatically generated extractive multi-document summarizations. While the results are good, they do rely on a certain type of ungrammaticality occuring often (missing spaces). Future work could concentrate on improving other grammaticality violations, so that not only the accuracy rises, but also the generality. Such a system could potentially detect ungrammaticality in a much broader domain.

We have also shown a simple but high-performing method of detecting datelines. We believe our results are close to what a human could achieve and don't see much room for improvement here, at least not with a variant of our method.

Finally we developed a basic method of detecting redundancy. With the lowest performance of all methods in this paper, redundancy detection is requiring much more work. While we based our approach only on the two subsequent sentences on which redundancy was to be tested, future work could include information available in the context of these sentences or even, as we did in \textbf{UnknownTokens-source}, information from the source documents.

One could also try following \cite{hatzivassiloglou1999detecting} closely and applying some form of machine learning as well, possibly similar to our \textbf{RandomForest} method.

\begin{table}
\begin{center}
\begin{tabular}{|l|c|c|c|}
  \hline
   & Precision & Recall & F-Score\\
  \hline
  Ungrammaticality & 72.8 & 49.1 & 58.6\\
  \hline
  Datelines & 86.0 & 89.7 & 87.8\\
  \hline
  Redundancy & 58.0 & 28.2 & 37.0\\
  \hline
  \end{tabular}
\end{center}
\label{eval_best}
\caption{The three best methods for detecting Ungrammaticality (\textit{other ungrammatical form} + \textit{incomplete sentence}), Datelines (\textit{inclusion of datelines}) and Redundancy (\textit{redundant information})}
\end{table}

The best methods for each violation, evaluated on \textit{test}, can be seen in Table~\ref{eval_best}.

Along with this paper, the authors will publish a toolchain that can be used to annotate given documents with the best methods described in here.

We only looked at a part of the violation types in \cite{friedrichlqvsumm}. Methods for detecting the last clause-level violation (no semantic relation), as well as the entity- and discourse-level violations have yet to be developed:

For the pronoun violations (\textit{pronoun with missing antecedent} and \textit{pronoun with misleading antecendent}), one could use the output of a coreference resolution tool such as BART \citep{versley2008bart} or the Stanford coreference resolution system \citep{lee2011stanford} to infer coreferences between entities in the summaries as well as the source documents and compare the originating coreference chains.

Finding \textit{acronym without explanation} seems to have a straightforward two-step solution: For every sequence of all-uppercase letters, find out whether those are an acronym that is not in a list of well-known acronyms. If it is, look for the expanded form of the acronym near its first occurence.

It is to be seen whether this method works as well as the author believes.

Going after the mention (\textit{first mention without explanation} and \textit{subsequent mention with explanation}) and noun phrase violations (\textit{definite noun phrase without reference to previous mention} and \textit{indefinite noun phrase with reference to previous mention}) would at least require a named entity recognition system, probably paired with at least some rudimentary form of semantic or discourse parsing.

\textit{no semantic relatedness} also suggests an approach using semantic parsing, but usable results might even be achieved by some method based on the distance between content words in WordNet \citep{fellbaum2005wordnet}.

Finally, the straightforward solution for detecting the \textit{no discourse relation} violation seems to be parsing the two sentences \textit{without} the discourse connective using a discourse parser, and then comparing the derived discourse relation with the relation commonly linked with the connective.

Future work could also include annotating a corpus after segmenting it, and using the adapted violation type system developed by us for that task.

And finally, since some of the characteristics of the violations our methods were tailored to are specific to the domain and corpus, it is yet to be seen how well our system performs on other corpora, other data sets and other domains.
Evaluation on anything other than extractive multi-document summarization systems however would probably lead to disappointing results: The violation types for which we have detection methods should not occur at all in abstractive systems, and some of them will likely not or only rarely occur in single-document summaries, for example \textit{redundant information} or some subtypes of \textit{other ungrammaticality}.

\newpage
\bibliographystyle{apalike}
\bibliography{referenzen}

\end{document}