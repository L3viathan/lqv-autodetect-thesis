\documentclass[a4paper,10pt]{scrartcl}
\usepackage{ngerman}
\usepackage{amsmath, amsfonts, epsfig, xspace}
\usepackage[utf8x]{inputenc}
\usepackage[english]{babel}
% For graphics
\usepackage[pdftex]{graphicx}
% you can place a figure at the position where it occurs in the text using [H]
\usepackage{here}

\usepackage{framed}
% For flexible tables
\usepackage{multirow}
% Math symbols not defined in the usual package, e.g. arrows that are crossed.
\usepackage{amssymb}

\usepackage{pbox}

% Arrows with text / superscript
\usepackage{amsmath}
% Adjust margin of paper.
\usepackage{geometry}
\geometry{a4paper, top=25mm, left=25mm, right=25mm, bottom=25mm}

% Zeilenabstand 1.25 %
\linespread{1.2}

\newcommand*{\SignatureAndDate}[2]{%
    \par\noindent\makebox[2.5in]{\hrulefill} \hfill\makebox[2.0in]{\hrulefill}%
    \par\noindent\makebox[2.5in][l]{#1}      \hfill\makebox[2.0in][l]{#2}%
}%

% Example Environments
\usepackage{amsthm}
\newtheoremstyle{style}   
  {0.5cm}              %Space above    
  {-0.8cm}              %Space below
  {}                      %Body font: original {\normalfont}    
  {}                      %Indent amount (empty = no indent,%\parindent = paraindent)    
  {\normalfont\bfseries}  %Thm head font original       
  {{\normalfont\bfseries \thmname{#1}\thmnumber{ #2}}}
\theoremstyle{style}
\newtheorem{example}{Example}[section]

% Formula Environments
\newtheorem{formula}{Formula}[section]

% Computational Linguistics trees etc.
\usepackage{xyling}

% Nicer captions
\usepackage{caption2}
\newcaptionstyle{mystyle}{%
  \normalcaptionparams
  \renewcommand\captionlabelfont{\bfseries}%
  \renewcommand\captionlabeldelim{.}%
  \onelinecaptionsfalse
  \usecaptionstyle{centerlast}}

\captionstyle{mystyle}

% Table of contents depth
\setcounter{tocdepth}{3}

% A horizontal rule for the title page
\newcommand{\HRule}{\rule{\linewidth}{0.5mm}}

% Paragraph and indent (as required by Prof. Dr. Pinkal)
\setlength{\parindent}{0pt}
\setlength{\parskip}{2ex plus 0.5ex minus 0.2ex}

\clubpenalty = 10000 % schliesst Schusterjungen aus 
\widowpenalty = 10000 \displaywidowpenalty = 10000% schliesst Hurenkinder aus 


\usepackage{color}

\usepackage{listings} %regex (listings)
\lstset{
    breaklines     = true,
    stepnumber     = 5,
    stringstyle    = \ttfamily,
    showstringspaces=true
}

\usepackage{rotating} %landscape tables
\usepackage{natbib} %schönere Zitate

\usepackage[colorinlistoftodos,textsize=tiny]{todonotes}

\presetkeys{todonotes}{fancyline}{}
\definecolor{todoorange}{rgb}{1, 0.8, 0.4}

\newcommand{\comment}[1]{
\todo[bordercolor=todoorange!80!black,color=todoorange]{\textbf{Comment:} #1}
}

\newcommand{\cn}{$^\text{[citation needed] }$}

\renewcommand\abstractname{\textbf{Abstract}}

\author{Jonathan Oberländer}

\title{Automatic Detection of Linguistic Quality Violations}

\begin{document}

\input{title.tex}

\thispagestyle{empty}
\section*{Erklärung}
Hiermit versichere ich, dass ich die vorliegende Arbeit selbstständig verfasst und keine anderen als die angegebenen Quellen und Hilfsmittel benutzt habe, dass alle Stellen der Arbeit, die wörtlich oder sinngemäß aus anderen Quellen übernommen wurden, als solche kenntlich gemacht und dass die Arbeit in gleicher oder ähnlicher Form noch keiner Prüfungsbehörde vorgelegt wurde.
\vspace{2cm}

\SignatureAndDate{Ort, Datum}{Unterschrift}

\vspace{5cm}

\section*{Declaration}

I hereby confirm that the thesis presented here is my own work, with all assistance acknowledged.
\vspace{2cm}

\SignatureAndDate{Place, Date}{Signature}
\newpage

\thispagestyle{empty}
\begin{abstract}
\setlength{\parskip}{2ex plus 0.5ex minus 0.2ex}
\textbf{Abstract} \quad Automatic summarization systems have in the past been evaluated mainly with regard to content coverage rather than linguistic quality factors, which leads to summaries containing many linguistic quality violations, like ungrammatical sentences.
Building on the work of \cite{friedrichlqvsumm}, who annotated automatic summaries with a set of violation types, we first identify a number of subtypes of ungrammaticality and annotate a subset of the data with these subtypes, finding that a large portion of ungrammaticality comes from two words missing spaces between them.
We develop methods for detecting the clause-level violations ungrammaticality, inclusion of datelines and redundancy automatically. For ungrammaticality, we employ a supervised tree learner using the output of a token-based detection approach for one of the subtypes, parser output and sentence length as features. We detect inclusion of datelines using regular expressions and redundancy using unigram overlap.
We obtain F-Scores of 58.6\% for ungrammaticality, 87.8\% for redundancy and 37.0\% for redundancy.
\end{abstract}

\begin{abstract}
\setlength{\parskip}{2ex plus 0.5ex minus 0.2ex}
\textbf{Acknowledgements} \quad I would like to thank my supervisors Prof. Dr. Manfred Pinkal, Annemarie Friedrich and Dr. Alexis Palmer for their continuous support in the process of creating this thesis, and for providing me with helpful comments and guidance whenever needed. This work couldn't have been done without the work previously done by Marina Valeeva. I also want to thank Lukas Burk for technical support.
\end{abstract}

\newpage

% Table of contents
\thispagestyle{empty}
\tableofcontents
\newpage

% Start of content
\setcounter{page}{1}		% Seitenzähler auf 1 setzen %
%\pagestyle{fancy}				% fancy header style
\pagenumbering{arabic}
\newpage

\maketableofcontents

\section{Introduction}
With an ever-growing amount of information, it is becoming increasingly important to reduce that information to a processible amount, by summarizing it. This is useful for people with time constraints who still want to read news, or for quickly skimming the content of a book, a website, a manual or a research paper. The scientific praxis of writing an abstract and an introduction doesn't just come out of nowhere, but is instead an effort to summarize the content of the paper at hand.
Most of the time though, at least when it comes to domains where writing a summary is not essential for the author, it is infeasible to do this manually, and there is thus a need for addressing the process with automated means.

Automatic summarization is the task of taking one or more documents as the input and producing a summary that contains the most important information from these documents.
\textit{Automatic summarization systems} can be divided in \textit{extractive} and \textit{abstractive}, and in \textit{single-document} and \textit{multi-document} summarization systems.
A \textit{single-document} system takes only one document as an input and produces a summary of that document, while a \textit{multi-document} system summarizes multiple documents on the same topic, while still producing only one summary.
An \textit{abstractive} system is one which infers knowledge from its input into an internal semantic representation and produces a novel text based on that knowledge, using language generation techniques. The task of constructing an abstractive summarization system is very complex, due to the aforementioned semantic representation. This is why most systems used today are \textit{extractive}.

Extractive summarization systems first select relevant sentences from the source document(s). They then reorder them and finally apply sentence fusion (combining multiple sentences into one) and sentence compression (reducing the size of a single sentence by leaving out parts deemed less relevant) to shrink the resulting summary.

We are only concerned with extractive, multi-document summarization, because our data, as described in Section~\ref{corpus_study}, consists of summaries produced by extractive multi-document summarization systems.

Evaluation of automatic summarization systems has largely been done on content coverage. Along with the task being hard in general, this is one of the reasons generated summaries often lack linguistic quality \citep{nenkova2011foundations}, meaning that they contain errors of various kinds. A new focus on linguistic quality (LQ) evaluation would presumably help decrease these issues and produce overall better summarization systems.

As a first step in this direction, \cite{friedrichlqvsumm} created the \textit{LQVSumm} corpus, which consists of multi-document summaries from the TAC 2011 shared task on Guided Summarization \citep{owczarzak2011overview}, annotated manually with LQ violations on entity and discourse level.
The corpus also includes annotations on 50 summaries produced by G-Flow \citep{gflow}, which we disregard, because the linguistic quality is much higher than in the rest of the corpus, along with the fact that the number of annotated G-Flow summaries is much smaller.

Manual annotation is an expensive and time-consuming process, for which an automatic system could be an alternative. Our work deals with creating such a system that manages at least part of the annotation automatically. We develop and evaluate our system on top of the \textit{LQVSumm} corpus. We also limit ourselves to the clause-level violations listed in \cite{friedrichlqvsumm}, except for \textit{no semantic relatedness} and \textit{no discourse relation}.

The remaining violations that we build detection methods for are \textit{other ungrammatical form} (e.g. \textit{A teenager at a school in Finland went on the rampage Wednesday,shooting dead seven classmates and the headmistress, having firstsignalled his intentions in a video posted on the Internet.}) and \textit{incomplete sentence} (e.g. \textit{Neighbors and family members saw no}), \textit{redundant information} (e.g. \textit{Iraqi reporter throws shoe at Bush in Baghdad. Iraqi reporter throws shoes at Bush in Baghdad.}) and \textit{inclusion of datelines} (e.g. \textit{MADRID, Feb. 15 (Xinhua) Rabei Ousmane Sayed Ahmed,}).

We find that we can divide ungrammaticality into a number of subtypes, which we annotate on a part of the corpus.
To detect ungrammaticality, we use decision trees with features extracted from parsing and a token-based, high-performing detection of the ungrammaticality subtype \textit{missing spaces}.
For detection of datelines, we use a regular expression. To detect redundant sentences, we use a simple n-gram overlap.

We achieve F-Scores of 58.6\% for ungrammaticality, 87.8\% for datelines and 37.0\% for redundancies.

The rest of this paper is organized as follows:

The next section discusses related work. In Section~\ref{corpus_study}, we take a look at the \textit{LQVSumm} corpus and the challenges it poses for our experiments and methods. We also demonstrate our subtype system in that section, to prepare for the experiments.

In Section~\ref{experiments} we describe and evaluate methods for annotating ungrammaticality, datelines, and redundancy.

We discuss our results in Section~\ref{discussion}. Finally, in Section~\ref{conclusion}, we discuss future work and conclude.

\newpage
\section{Related Work}
\label{related_work}
% other works on grammaticality and so on

\subsection{Ungrammaticality detection}

Pioneer work on the topic of ungrammaticality detection was done by \cite{atwell1987detect}, who used the part-of-speech (POS) tagger CLAWS \citep{leech1983automatic} alongside learned frequencies/likelihood of POS tag pairs in grammatical sentences to annotate errors using Markov chains.
They then add error tags to words in their tag dictionary --- POS tags of words spelled similar to some word were specially marked on that word, when not in the POS tags of the word itself  --- and use them to make finding likely typing errors easier.

Most work in the field of automatic grammar checking has been done in the area of second language learning.
\cite{bender2004arboretum} use the LKB parser and generator \citep{copestake2002implementing} with the English Resource Grammar \citep{flickinger2000building} to build \textit{Arboretum}, a system that, when given an erroneous input sentence, detects that error and outputs a corrected version of the sentence. Their method uses manually selected mal-rules \citep{schneider1998recognizing} to detect error types specific to second language learners of English.

While this seems to work for their specific domain, it does not transform well into ours: The mistakes language learners make, and the kind of ungrammaticality summarization systems produce are very different. Source texts that are being summarized are usually grammatical, and get their ungrammaticality during summarization from being compressed from two formerly grammatical sentences into one ungrammatical one, for example. Mistakes typical for learners of English as a second language either usually do not occur in automatic summarizations (such as misspelled words), or do happen, but due to a different cause (for example agreement). Like \cite{bender2004arboretum}, we're using the English Resource Grammar, for our parser features in Section~\ref{s_randomforestfull}.

\cite{chodorow2000unsupervised} build \textit{ALEK}, a system to grade \textit{TOEFL} essays. It does so by viewing the task as an extension of Word Sense Disambiguation, and collecting n-grams of POS-tags and function words, and then classifying using primitive statistical measures like \textit{Mutual Information} and the $\chi^2$ test. However, only the whole essay is classified into one of six possible scores, and this decision is only made by looking at the test subject's understanding of the correct usage of the three words \textit{Concentrate}, \textit{Interest} and \textit{Knowledge} in context.
Even more than \cite{bender2004arboretum}, this approach is not applicable to our task, because we're seeking to classify ungrammaticality per sentence and not just per essay/summary. In addition, the method won't transform well to our domain of news texts, where we can't rely on a few selected words to occur.

\cite{sun2007detecting}, while also citing second language learning as their primary application, use a different approach: Instead of manually writing rules, they employ a supervised statistical learning method. They extract what they call \textit{labeled sequential patterns} (ngrams associated with a class) from both erroneous and correct sentences in their training corpus, enrich them with other automatically computed features, and use them as input for classification models; an SVM \citep[SVMlight,][]{Joachims/02a} and a Na\"ive Bayes classification model. The system achieves F-scores of around 80\%, evaluated on correct and erroneous corpora of second language learning.

\cite{tetreault2010using} build on a Maximum Entropy classifier using 25 lexical features, and add 14 parsing-based features to detect errors in preposition usage of learners of English as a second language. On a test corpus extracted from TOEFL test data, they achieved a precision of 48.6\% with a recall of 22.5\%.

There also has been work in the area of assessing grammaticality of automatically generated summaries, namely \cite{vadlapudi2010automated}. They used n-grams on POS-tag level and evaluated their system by looking at how well their classification correlates to \textit{ROGUE} scores \citep{lin2004rouge}. \citeauthor{vadlapudi2010automated} achieve a Spearman's $\rho{}$ correlation of up to 77\% to ROGUE scores manually assigned by annotators.

The main inspiration for our work on detecting ungrammaticality comes from \cite{wagner2007comparative}, who compare various methods for detecting ungrammaticality:

\begin{itemize}
	\item \textbf{Precision Grammar:} After parsing a sentence with the XLE LFG parser \citep{maxwell1996efficient}, a sentence is classified as ungrammatical based on the *-mark (which the parser assigns to any sentence whose optimal solution uses a constraint marked as ungrammatical), parser exceptions during runtime and on whether there is a parse at all.
	\item \textbf{POS N-grams:} Using 10-fold cross validation, part-of-speech n-grams ($2 \leq n \leq 7$) are counted on the training data, and a sentence is classified as ungrammatical if it contains an n-gram that is below a threshold.
	\item \textbf{Decision Trees on XLE Output:} Using parser output as features (including whether a sentence is marked with a * (see above), the number of words and the number of parses, amongst others), a J48 decision tree classifier is trained.
	\item \textbf{Decision Trees on N-grams:} The same experimental setup as in the previous method, but using the frequencies of the rarest n-gram for each possible n ($2 \leq n \leq 7$).
	\item \textbf{Decision Trees on Combined Feature Sets:} Using the features from the two previous methods.
\end{itemize}

Their error corpus is an artificial one, generated by introducing systematical errors into otherwise grammatical data. Their best method (\textbf{Decision Trees on Combined Feature Sets}) achieves an F-Score of 66.9\%. In the method described in Section~\ref{s_randomforestfull}, we follow their idea of using decision trees with parser output as a feature, but include different additional features that exploit the characteristics of automatic summaries.

\subsection{Redundancy detection}

The \textit{BLEU} score \citep{papineni2002bleu} is a method for evaluating the effectiveness of a machine translation system, which uses a number of (human) reference translations to score an automatic translation. They showed that their score correlates highly with human evaluation, which has lead to the \textit{BLEU} score having become the standard evaluation metric for machine translation.
This is not designed to detect redundancy, but as BLEU scores translations by comparing them to (human) reference translations, it could potentially be used to detect redundant sentences as well.

\cite{hatzivassiloglou1999detecting} implement a system for redundancy detection on the level of ``text units'' (paragraphs or sentences) using machine learning techniques. As primitive features, they use:

\begin{itemize}
	\item \textbf{Word co-occurence:} A very similar feature to our \textbf{Unigrams} approach in Section~\ref{s_unigrams}.
	\item \textbf{Matching noun phrases:} Using \textit{LinkIt} \citep{wacholder1998simplex}, they detect simplex noun phrases and match those sharing a head
	\item \textbf{WordNet synonyms:} Matching words that occur in the same \textit{WordNet} \citep{fellbaum2005wordnet} synset.
	\item \textbf{Common semantic classes for verbs.} Matching verbs that share the same semantic class \citep{levin1993english}
	\item \textbf{Shared proper nouns:} Matching shared proper nouns, identified by \textit{Alembic} \citep{aberdeen1995mitre}
\end{itemize}

These features were then optionally normalized by sentence length or relative frequency of the feature occurence.

In addition to these primitive features, they also used three composite features, which combine pairs of primitive features and are applied as restrictions on their primitive features:

\begin{itemize}
	\item \textbf{Order:} Whether an element (word) pair had the same relative order in both sentences/textual units, i.e. word X comes before word Y in both textual units, or vice versa, for some words X and Y
	\item \textbf{Distance:} Whether two elements are no farther than some value in both textual units
	\item \textbf{Primitive:} Elements are restricted to a specific primitive feature
\end{itemize}

They then trained a machine learning algorithm \citep{cohen1996learning} and evaluated on a news text corpus, for which they achieved a precision of 60.5\% and a recall of 36.6\%.

\newpage
\section{Corpus}
\label{corpus_study}
% describe the corpus and its flaws
The LQVSumm corpus consists of annotations on summaries produced by systems participating in the TAC 2011 Guided Summarization Task \citep{owczarzak2011overview}. The source documents for the generated summaries are news articles.
The corpus contains 44 ``topics'', each of which contains 44 summaries.
Summaries in the same topic are based on the same source documents.

\subsection{Violation types}

Some of these annotations were done on the \textit{entity level}, others on the \textit{clause level}. Our work focusses on four clause-level violations.

When a clause is annotated with an \textit{incomplete sentence} violation (INCOMPLSN), the sentence was cut off, most likely due to a summarization system trying to fit as much in the summary as the limit for the task allowed, or due to sentence compression. An example of such a clause would be:

\begin{framed}
\quad\textit{Neighbors and family members saw no}
\end{framed}

Annotations of the violation type \textit{inclusion of datelines} (INCLDATE) mark timestamps that have ended up in the summary. These fragments are the result of the summaries' domain: news articles. Datelines appear in a couple of different variations, most of the time at the beginning of a sentence:

\begin{framed}
\quad\textit{MADRID, Feb. 15 (Xinhua) Rabei Ousmane Sayed Ahmed,}
\end{framed}

This example also shows that multiple violations can of course happen in the same sentence, as it is also tagged as containing an \textit{incomplete sentence} violation.

The violation type \textit{other ungrammatical form} is used for any other ungrammaticality happening in a single phrase. Therefore, we find the biggest variety of violations in here:

\begin{framed}
\quad\textit{Valley Veterinary Bismarck, North Dakota, a veterinarian, Carlson, has been worried that pet owners to the Menu Foods on its web site.}

\quad\textit{A Boeing 737-400 plane with 102 people on board crashed into amountain in Indonesia's West Sulawesi province on Monday, killingat least 90 people.}

\quad\textit{//www.menufoods.com/recall -- early Saturday.}
\end{framed}

The remaining three violation types apply to multiple phrases. A \textit{redundant information} violation (REDUNINF) means two phrases share a lot or all of their information. There are instances of this violation where the two phrases are exactly the same, and others where the information is rephrased in the second sentence:

\begin{framed}
\quad\textit{\textbf{Representatives} from \textbf{the commercial fishing industry, environmental groups and government regulators} are discussing \textbf{ways to strengthen information sharing and cooperation among regional organizations to} track and \textbf{manage tuna} stocks.}

\quad\textit{\textbf{Representatives}, including \textbf{the commercial fishing industry, government regulators and environmental groups}, plan to discuss \textbf{ways to strengthen information sharing and cooperation among regional organizations to} better \textbf{manage tuna} stock and adopt an action plan, the Japanese Fisheries Agency said in a statement.}
\end{framed}

Because the instance counts for the last remaining violation types (\textit{no semantic relatedness} and \textit{no discourse relation}) were so low (only 142 instances of \textit{no semantic relatedness} and 91 instances of \textit{no discourse relation}), they were disregarded and we didn't try to a develop methods to detect them.
Suggestions for possible detection methods for the omitted violation types are made in Section~\ref{conclusion}.

\subsection{Experimental Setup}
The annotation of LQVSumm was done using the MAE annotation tool \citep{stubbs2011mae}. On top of these annotations, further annotations done from our side are applied to the existing files, but in separate tags splitting corpus data from classification or added features.

We split the corpus into three parts, while making sure that topics weren't spread across sets: \textit{dev-1} and \textit{dev-2} are development sets, \textit{test} is our test set. We have two development sets, because for some methods, we developed on one part and trained on another.
\textit{dev-1} contains 351 summaries, \textit{dev-2} contains 352, that is 8 topics per development set and about 20\% of the corpus each.
The test set \textit{test} contains the other 60\%, making up 1232 summaries and 28 topics.

The breakdown of the distribution of topics across development and test sets is shown in Table~1.%\ref{testsets}.

\begin{table}[H]
\begin{center}
\begin{tabular}{|c|c|c|}
\hline
\textbf{Set} & \textbf{Number of summaries} & \textbf{Topics}\\
\hline
\textit{dev-1} & 351 & D1101 -- D1108\\
\hline
\textit{dev-2} & 352 & D1109 -- D1116\\
\hline
\textit{test} & 1232 & D1117 -- D1144\\
\hline\hline
Overall & 1935 & D1101 -- D1144\\
\hline
\end{tabular}
\end{center}
\label{testsets}
\caption{Distribution and count of summaries across development and test sets}
\end{table}

Our annotations, experiments and the evaluation are all done on sentence basis. We use Stanford CoreNLP \citep{manning-EtAl:2014:P14-5} for sentence splitting. This is necessary to have classifiable entities, but introduces inaccuracies, because clause-level violations in LQVSumm are annotated on phrase level, phrase meaning an arbitrary substring of a summary, which is often a sentence, but also sometimes spanning over sentence bounds.

\subsection{Subtype system}
\label{s_subtypes}

As a first step, we inspect the corpus. We find that the type system of the LQVSumm corpus \citep{friedrichlqvsumm} is not detailed enough for our purposes and has a few shortcomings: Especially under the label of \textit{other ungrammatical form}, we see many different types of errors combined.

Since we assume that dividing the problem into smaller, easier problems could help us detect \textit{other ungrammatical form} violations, we decide to define a number of subtypes for this violation:

\begin{description}
\item[missing spaces] \hfill \\
	This is the most common type of error: The sentence contains a word that does not exist. In almost all cases, this happens when whitespace between two words is missing.

	Example: \textit{A strong earthquake measuring 7.8 magnitude struck \\\textbf{Wenchuancounty} of Sichuan Province on Monday, leaving at least \\\textbf{12,000people} died and thousands more injured.}

\item[missing words] \hfill \\
	One or multiple words are clearly missing. These seem to most often be function words such as articles or pronouns, rather than content words. In the example, an underscore marks the position where a word, probably ``was'' is missing.

	Example: \textit{An Israeli woman \_ killed and 11 others were wounded in the suicide bombing at a shopping mall in southern Israeli town of Dimona.}

\item[punctuation error] \hfill \\
	Most of the time, this means: Punctuation is missing, but it can also mean there is something else wrong with punctuation in that sentence which makes it ungrammatical, as can be seen in the example: Unbalanced parantheses.

	Example: \textit{China has allocated 200 million yuan (million dollars for disaster relief work after an earthquake rocked the country's killing at least seven people, state reported on Tuesday.}

\item[capitalization error] \hfill \\
	Words that should be capitalized are not or words that should not be capitalized are.

	Example: \textit{earlier on \textbf{m}onday \textbf{g}erman chancellor \textbf{a}ngela \textbf{m}erkel and foreign \\minister \textbf{f}rank \textbf{w}alter \textbf{s}teinmeier offered their condolences to \textbf{c}hina over the heavy loss of life in the powerful earthquake that hit \textbf{c}hina's southwestern province of \textbf{s}ichuan.}

\item[unparsable]\hfill \\
	This subtype means that the (human) annotator was unable to make sense of the sentence and considers it to be not correctly parsable. Most instances of this subtype seem to happen when sentence fusion occurs; the summarization system combined two or more sentences in an ungrammatical or meaningless way.
	%This subtype looks similar to a punctuation error, but differs in that sentences are intermixed with each other; in the middle of one sentence, the reader suddenly finds themself in a different one. This could also happen if part of a sentence was removed. In the example, an underscore marks the point at which the break happens.

	Example: \textit{All of those provinces and Chongqing, a special municipality \_ deepest condolences to those who lost their loved ones in the devastating natural disaster.}

\item[heading] \hfill \\
	The sentence contain (usually at the beginning) a sequence of capitalized or otherwise heading-like words that aren't part of a dateline.

	Example: \textit{THE CURRENT FIX: Internet applications such as firewalls and spam filters attempt to control security threats.}

\item[incomplete sentence] \hfill \\
	The type system of the LQVSumm corpus \citep{friedrichlqvsumm} defines an \textit{incomplete sentence} violation as words being cut off at the end of a sentence. A couple of times though, this also occurs at the beginning of a sentence. We restructure the type system by treating all types of incomplete sentences as this subtype of \textit{other ungrammatical form}.

	Example: \textit{A Palestinian suicide bomber detonated an explosive belt at a \\commercial center in Dimona on}.
\item[should be other type] \hfill \\
	Sometimes sentences that are tagged as \textit{other ungrammaticality} should (also) be tagged as another type, such as sentences containing datelines. This doesn't necessarily mean that the sentence contains no other ungrammaticalities. This subtype be caused by inconsistencies in the corpus, or from the process of mapping the annotated violations to sentences. While the following example is an \textit{incomplete sentence}, it also contains a dateline.

	Example: \textit{its deepest sympathy to the Israeli people,\textbf{00 a.m. local time (0800 GMT)} as a suicide bomber detonated an explosive belt in the commercial center, according to local media reports.}
%\item[not ungrammatical] \hfill \\
%	As is to be expected, the annotation of the corpus isn't correct 100\% of the time. This subtype merely denotes a correct sentence that was incorrectly marked as ungrammatical.
%
%	Example: \textit{One Israeli woman was killed and at least eight others wounded on Monday in a suicide bombing which ripped through a commercial center in the southern Israeli town of Dimona, the first attack of the kind since January 2007.}

%%% did not occur in dev-2

\end{description}

After manually counting and investigating these subtypes on \textit{dev-1} we then mark these types in \textit{dev-2} for every sentence that is tagged as \textit{other ungrammatical form} in the corpus. We allow sentences to have multiple subtypes and also annotate \textit{incomplete sentence} violations with additional (sub)types, if applicable. The frequency distribution of the subtypes in \textit{dev-2} can be seen in Figure~\ref{subtypes}.

Perhaps surprisingly, not all subtypes occur with the same frequency, in fact, one of them makes up almost half of the ungrammaticality violations. We exploit that fact in Section~\ref{sec_unknowntokens} by building a detection method specifically for that subtype.

It is to be noted, that from here on, we consider \textit{incomplete sentence} to be a subtype of \textit{other ungrammatical form}.

\begin{figure}
\begin{center}
\includegraphics[scale=0.6]{subtypes_dev2.png}
\end{center}
\caption{\textbf{Subtypes of ungrammaticality in \textit{dev-2}.} Where the annotator didn't know what subtype to annotate the sentence with, they selected ``other''.}
\label{subtypes}
\end{figure}

\newpage
\section{Experiments}
\label{experiments}
% All experiments, intermixed with small discussion segments
We conduct a number of experiments to find automatic methods for detecting linguistic quality violations on the clause level. We focus on ungrammaticality, datelines and redundancy and propose methods based on a variety of syntactic features as well as features from language models and the output statistics of an HPSG parser.

We use the standard metrics of Precision, Recall, F-Score, and Accuracy, which can be seen in Figure~\ref{metrics}. In general, we focus on Precision: We consider missing an violation (false negatives) to be far more acceptable than accidentally mislabeling a non-violation (false positives), because given a high recall, even with a very low precision, we don't add any erroneous information, if the list of classified violations is not considered to be necessarily exhaustive. We assume a large number of false positives would lower the acceptance by people using our system.

\begin{figure}[H]
	\begin{subequations*}
	\begin{align*}
		\text{Precision} &= \frac{\text{true positives}}{\text{true positives} + \text{false positives}}\\
		\text{Recall} &= \frac{\text{true positives}}{\text{true positives} + \text{false negatives}}\\
		\text{F-Score} &= 2*\frac{\text{precision}*\text{recall}}{\text{precision}+\text{recall}}\\
		\text{Accuracy} &= \frac{\text{true positives}+\text{true negatives}}{\text{all instances}}
	\end{align*}
	\end{subequations*}
	\label{metrics}
	\caption{Standard metrics Precision, Recall, F-Score and Accuracy}
\end{figure}

\newpage
\subsection{Detecting Ungrammaticality}
\label{sec_unknowntokens}
We start with the violation type \textit{other ungrammatical type} (plus \textit{incomplete sentence}, as explained in Section~\ref{s_subtypes}).

Perhaps the most noticeable result of our corpus study is the large portion of \textit{missing spaces} violations, meaning clauses that include tokens which aren't correct words. Almost all of these cases come from a missing space between two words forming tokens such as \textit{reportsreaching} or \textit{Wenchuancounty}. As there is such a large amount of this type, we believe finding a reliable detection method for this subtype would presumably significantly boost detection of ungrammaticality in general. That method, \textbf{UnknownTokens}, is explained in the following section.

\subsubsection{The UnknownTokens method}
We call our approach to identifying missing spaces \textbf{UnknownTokens}: Tagging a sentence as containing a \textit{missing spaces} ungrammaticality if and only if there is a token in the sentence that isn't a ``known'' token, i.e. one that doesn't exist in a list of known words. In our setting, we can easily deduce such a list from the source documents of the automatic summarization process. If the source document is available and used as a base for our vocabulary, we call that approach \textbf{UnknownTokens-source}.

In case the original documents aren't available or this method should be used in a different setting, we also investigate how to adapt our approach to that. This approach has a few steps, which we evaluate individually:

\begin{itemize}
\item First, an initial list of words is created from a large text corpus (\textbf{UnknownTokens-gw}); we use the first 20\% of the AFE part of the gigaword corpus \citep{gigaword}.

\item Because this will not include words for named entities yet, we only consider a token to be an unknown token if it is not tagged as a Named Entity by a Named Entity Recognizer (\textbf{UnknownTokens-ner}). We employ the Stanford Named Entity Recognizer \citep{stanfordNER}, which is a widely used, state-of-the-art NER that comes with a model for the English language.

\item In order to improve upon the NER and further increase recall, we use two simple heuristics (\textbf{UnknownTokens-heur}): Firstly, we assume that words that start with a capital letter and are only followed by lower case letters are named entities and therefore no unknown tokens. We assume words containing hyphens to denote named entities as well. 

\item Finally, we automatically check whether a token has an entry in the free online encyclopedia Wikipedia (\textbf{UnknownTokens-wiki}).

\end{itemize}

We automatically label any tokens that are neither on our list of known tokens from GigaWord, nor tagged as a named entity as ``unknown'' or caught by either our heuristic or the wikipedia-based addition and tag the sentence as containing unknown tokens.

\subsubsection{Evaluation of UnknownTokens}
We evaluate how effective \textbf{UnknownTokens} is in detecting \textit{missing spaces} violations, both with\\ (\textbf{UnknownTokens-source}) and without (\textbf{UnknownTokens-gw+heur+ner+wiki}) access to the source documents, and compare it to a \textbf{baseline}, which always assumes the most common case (no \textit{missing spaces} violation).
The standard metrics precision, recall and F-score (see Figure~\ref{metrics}) are used for evaluation.

Table~\ref{eval_unknowntokens} shows the results of the evaluation of both variations of \textbf{UnknownTokens} on \textit{dev-2}. The weighted metric ``Macro-average'' is provided, too.
% The micro-average method consists of summing up the individual values from the confusion matrix (True Positive, False Positive, True Negative and False Negative) and calculating new metrics based on the results. We only display the F-score here, since F-score, recall and precision are identical for a 2-class scenario: Since the true positives of the missing spaces class are identical with the true negatives of the no missing spaces class (and vice versa), the new number of true positives and true negatives (for the micro average) is $tp + tn$ of any of the two classes. 
% The same goes for false negatives and false positives. That means that when only dealing with two classes, in the micro-average method, the terms \textit{positive} and \textit{negative} can be used interchangeably. As precision is defined as $\frac{tp}{tp+fp}$ and recall as $\frac{tp}{tp+fn}$; as the equations only differ in whether they use $fp$ or $fn$ (which are exactly the same here) in the divisor, precision and recall (and therefore F-score, too, since the F-score is simply the harmonic mean of precision and recall) are identical.
Macro-averages are the arithmetic means of precision and recall of both classes. The new F-score is calculated from the new scores.

Both \textbf{UnknownTokens-gw+heur+ner+wiki} and \textbf{UnknownTokens-source} improve vastly over the baseline, but it is clear that if the source text is available, it should be used to infer known words. We note an increase in F-score of 20.5 by adding our heuristics to \textbf{UnknownTokens-gw}. Adding the information from the Named Entity Recognizer increases it by another 5.5. Finally, by adding the check in wikipedia, we get another 29.2 on top of that.
In contrast, using just the tokens from the source documents (\textbf{UnknownTokens-source}) yields an F-score 14 higher than that of \textbf{UnknownTokens-gw+heur+ner+wiki}.

It is evident from the high accuracy of the baseline that the vast majority of sentences does not contain the violation.

%%%BEGIN BACKUP WITHOUT WEIGHTED MEAN:
\begin{sidewaystable}
\begin{center}
\begin{tabular}{|r|c|c|c|c|c|c|c|c|}
\hline
& \multicolumn{3}{c|}{Missing spaces} & \multicolumn{3}{c|}{No missing spaces} & Macro-average & Accuracy\\
& \textbf{P} & \textbf{R} & \textbf{F} & \textbf{P} & \textbf{R} & \textbf{F} & \textbf{F} & \\
\hline
\textbf{Baseline} & 0.0 & 0.0 & 0.0 & 94.8 & \textbf{100} & 97.3 & 48.7 & 94.8\\
\hline
\textbf{UT-gw} & 15.0 & \textbf{98.7} & 26.0 & \textbf{99.9} & 69.1 & 81.7 & 68.2 & 70.7\\
\hline
\textbf{UTgw+heur} & 30.5 & 97.3 & 46.5 & 99.8 & 87.8 & 93.4 & 76.5 & 88.3\\
\hline
\textbf{UT-gw+heur+ner} & 35.5 & 97.3 & 52.0 & 99.8 & 90.3 & 94.8 & 78.6 & 90.6\\
\hline
\textbf{UT-gw+heur+ner+wiki} & 70.3 & 96.0 & 81.2 & 99.8 & 97.8 & 98.8 & 90.6 & 97.7\\
\hline
\textbf{UT-source} & \textbf{95.9} & 94.6 & \textbf{95.2} & 99.7 & 99.7 & \textbf{99.7} & \textbf{97.5} & \textbf{99.5}\\
\hline
\end{tabular}
\end{center}
\caption{Evaluation of \textbf{UnknownTokens} on \textit{dev-2}, in percent. \textbf{gw} = Gigaword-based dictionary, \textbf{heur} = title case heuristics, \textbf{ner} = Stanford NER, \textbf{wiki} = Wikipedia entry check}
\label{eval_unknowntokens}
\end{sidewaystable}
%%%END BACKUP WITHOUT WEIGHTED MEAN

%%%BACKUP WITH WEIGHTED MEAN:
% \begin{sidewaystable}
% \begin{center}
% \begin{tabular}{r|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|}
% & \multicolumn{3}{c|}{Positive Set} & \multicolumn{3}{c|}{Negative Set} & \multicolumn{3}{c|}{Weighted Mean} & Micro-average & \multicolumn{3}{c|}{Macro-average}\\
% & \textbf{P} & \textbf{R} & \textbf{F} & \textbf{P} & \textbf{R} & \textbf{F} & \textbf{P} & \textbf{R} & \textbf{F} & \textbf{F} & \textbf{P} & \textbf{R} & \textbf{F}\\
% \hline
% \textbf{Baseline} & 0.0 & 0.0 & 0.0 & 94.79 & 100 & 97.33 & 89.85 & 94.79 & 92.26 & 94.79 & 47.40 & 50.0 & 48.67\\
% \hline
% \textbf{UT\textsubscript{wn}} & 14.96 & 98.65 & 25.98 & 99.89 & 69.14 & 81.72 & 95.47 & 70.68 & 78.82 & 70.68 & 57.43 & 83.90 & 68.19\\
% \hline
% \textbf{UT\textsubscript{wn+heur}} & 30.51 & 97.30 & 46.45 & 99.83 & 87.81 & 93.43 & 96.22 & 88.30 & 90.98 & 88.30 & 65.17 & 92.56 & 76.49\\
% \hline
% \textbf{UT\textsubscript{wn+heur+ner}} & 35.47 & 97.30 & 51.99 & 99.84 & 90.26 & 94.81 & 96.49 & 90.63 & 92.58 & 90.63 & 67.66 & 93.78 & 78.61\\
% \hline
% \textbf{UT\textsubscript{wn+heur+ner+wiki}} & 70.30 & 95.95 & 81.15 & 99.77 & 97.77 & 98.76 & 98.23 & 97.68 & 97.84 & 97.67 & 85.04 & 96.86 & 90.57\\
% \hline
% \textbf{UT\textsubscript{source}} & 95.89 & 94.59 & 95.24 & 99.70 & 99.70 & 99.70 & 99.50 & 99.43 & 99.47 & 99.50 & 97.80 & 97.15 & 97.47\\


% \end{tabular}
% \end{center}
% \caption{Evaluation of \textbf{UnknownTokens} on \textit{dev-2}, in percent. \textbf{UT\textsubscript{wordnet+heur+ner+wiki}} is equivalent to \textbf{UnknownTokens\textsubscript{general}}. \textbf{\textsubscript{wn}} = Wordnet-based dictionary, \textbf{\textsubscript{heur}} = title case heuristics, \textbf{\textsubscript{ner}} = Stanford NER, \textbf{\textsubscript{wiki}} = Wikipedia entry check}
% \label{eval_unknowntokens}
% \end{sidewaystable}
%%%END BACKUP WITH WEIGHTED MEAN


\subsubsection{The RandomForest-LM+UT+words method}
\label{s_randomforest}
To detect more general forms of ungrammaticality, several other methods were investigated. Following \cite{wagner2007comparative}, we train decision trees from outputs of several systems that on their own have not proven sufficient for this task. %or this experiment, the development set \textit{dev-1} was used.
This experiment was then evaluated on \textit{test}.

Using SRILM~\citep{stolcke2002srilm}, we train a language model on parts (the same parts we used as a word source in \textbf{UnknownTokens-gw+heur+ner+wiki} (section~\ref{sec_unknowntokens})) of the Gigaword corpus \citep{gigaword}, extract perplexity scores for the sentences in our test set, and annotate them automatically. Perplexity scores can be described as how surprised the system is when seeing the end of a test sentence, based on the information in the language model.
The perplexity scores alone had shown not to be effective in detecting ungrammaticality.

%The free WEKA software~\citep{hall2009weka} is an allround machine learning toolkit.
We then use a RandomForest \citep{breiman2001random} to learn decision trees, utilizing the machine learning toolkit WEKA \citep{hall2009weka}.
RandomForest works by randomly selecting subsets of training instances along with random subsets of features to train a number of decision trees and then classifying test instances with what the majority of trees are classifying the instance with.

%As an input, WEKA requires an ARFF file, which we created using three features:
We use three features for building our classifier:

\begin{itemize}
	\item \textbf{language model perplexity} is the perplexity calculated for a sentence by the language model, following \cite{sun2007detecting}. 
	This is a numeric feature.
	\item \textbf{missing spaces} is the classification of \textbf{UnknownTokens-source} for a given sentence, as described in section~\ref{sec_unknowntokens}. This is a binary feature --- either a sentence was classified as containing the violation, or as not containing it.
	\item \textbf{numberofwords} is the number of words in the sentence. We follow \cite{wagner2007comparative} here, the intuition is that ungrammatical sentences will often be extraordinarily long, or very short:
		\begin{framed}
		\textit{Former US President George W Bush, Who Will attend a memorial service on Tuesday, Virginia, and the university of at least 33 people Were killed, in the Worst shooting rampage in modern history, the White House said.}

		\textit{The report of a}
		\end{framed}
\end{itemize}

% \subsubsection{Evaluation of RandomForest-UT}
% \label{S_eval_randomforest}

% We performed 10-fold cross-validation, which lead to a weighted F-score of 86.3\%. A total of 1215 (88.1\%) instances was classified correctly, 164 (11.9\%) incorrectly.

% === Run information ===

% Scheme:       weka.classifiers.trees.RandomForest -I 10 -K 0 -S 1 -num-slots 1
% Relation:     ungrammaticality
% Instances:    1379
% Attributes:   4
%               language_model
%               nonwords
%               numberofwords
%               class
% Test mode:    10-fold cross-validation

% === Classifier model (full training set) ===

% Random forest of 10 trees, each constructed while considering 3 random features.
% Out of bag error: 0.1291



% Time taken to build model: 0.43 seconds

% === Stratified cross-validation ===
% === Summary ===

% Correctly Classified Instances        1215               88.1073 %
% Incorrectly Classified Instances       164               11.8927 %
% Kappa statistic                          0.3351
% Mean absolute error                      0.1491
% Root mean squared error                  0.3094
% Relative absolute error                 65.2325 %
% Root relative squared error             91.6119 %
% Coverage of cases (0.95 level)          96.2292 %
% Mean rel. region size (0.95 level)      66.4612 %
% Total Number of Instances             1379     

% === Detailed Accuracy By Class ===

%                  TP Rate  FP Rate  Precision  Recall   F-Measure  MCC      ROC Area  PRC Area  Class
%                  0.970    0.707    0.901      0.970    0.934      0.361    0.766     0.938     ungrammaticality-no
%                  0.293    0.030    0.596      0.293    0.393      0.361    0.766     0.417     ungrammaticality-yes
% Weighted Avg.    0.881    0.618    0.861      0.881    0.863      0.361    0.766     0.869     

% === Confusion Matrix ===

%     a    b   <-- classified as
%  1162   36 |    a = ungrammaticality-no
%   128   53 |    b = ungrammaticality-yes

\subsubsection{The RandomForest-LM+UT+words+parser method}
\label{s_randomforestfull}
We also investigated whether parser output would improve the results. For this task, we utilized the HPSG parser Answer Constraint Engine \citep{ace} with the English Resource Grammar \citep{copestake2002implementing} to parse the entire corpus and annotate it with output statistics of the parser: Following \cite{wagner2007comparative} loosely, we extracted the number of readings/parses, the amount of memory the parser used for the parse and the success status, which could be either ``ok'' or ``ignored'', the latter appearing when the parsing failed entirely for some reason.

We then added these three features to \textbf{RandomForest-LM+UT+words} and call the new method \textbf{RandomForest-LM+UT+words+parser}.

\subsubsection{Evaluation of RandomForest-LM+UT+words+parser}

\begin{table}
\begin{center}
\begin{tabular}{|r|c|c|c|c|}
  \hline
  & \textbf{Precision} & \textbf{Recall} & \textbf{F-Score} & \textbf{Accuracy}\\
  \hline
  Baseline & - & 0 & - & 77.7\\
  \hline
  RandomForest-LM+UT+words & 65.5 & 44.2 & 52.8 & 82.3\\
  \hline
  RandomForest-LM+UT+words+parser & \textbf{72.8} & \textbf{49.1} & \textbf{58.6} & \textbf{84.5}\\
  \hline
  \end{tabular}
\end{center}
\caption{10-fold cross validation of \textbf{RandomForest-LM+UT+words} and \textbf{RandomForest-LM+UT+words+parser} on \textit{test}, in percent, based on the class of ungrammatical sentences. As Baseline, we classified every sentence with the most-frequent class, ``not ungrammatical''.}
\label{eval_randomforest}
\end{table}


We performed 10-fold cross-validation with both \textbf{RandomForest-LM+UT+words} and \textbf{RandomForest-LM+UT+words+parser} on \textit{test}, the results of which can be seen in Table~\ref{eval_randomforest}. These evaluations are done on the class of ungrammatical sentences, additionally evaluating it on the class of grammatical sentences leads to much higher scores (with a weighted F-score of 83.4\% for both ``ungrammatical'' and ``not ungrammatical'').

As evident from the table, including the parser output statistics as features (\textbf{RandomForest-LM+UT+} \textbf{words+parser}) significantly improves upon the performance of just \textbf{RandomForest-LM+UT+words}. We generally value precision more than recall, because we prefer if our system misses a case of ungrammaticality to it falsely classifying a grammatical sentence as ungrammatical.

\begin{table}
\begin{center}
\begin{tabular}{c|c}
  \hline\hline
  Feature & Decrease in F-Score\\%84.5391%0.586
  \hline
  Parser Readings & 5.7\\%478\\0.586-0.529
  Number of Words & 2.2\\%34\\0.586-0.564
  Language Model Perplexity & 1.4\\%1.2091\\0.586-0.572
  Parser RAM & 1.3\\%01\\0.586-0.573
  UnknownTokens Output & 0.8\\%0.3369\\0.586-0.578
  Parser Status & 0.4\\%1982\\0.586-0.582
  \hline\hline
  \end{tabular}
  \end{center}

  \caption{Ablation study of \textbf{RandomForest-LM+UT+words+parser}. The numbers denote the decrease of the F-Score if that feature is taken out.}
  \label{ablation_study_randomforest}
 \end{table}

An ablation study can be seen in Table~\ref{ablation_study_randomforest}.
The numbers in the table describe by how much the F-score goes down if we were to remove a specific feature.
No number (except for \textit{Parser Readings}, the most effective feature) being very high indicates that we have some redundancy of information in the features here; none of the other features have a lot of \textit{exclusive} information about ungrammaticality.

In this case, we consider redundancy to be a good thing, as it should make the output more stable.

%When evaluating the emerging decision tree on general ungrammaticality using the same method as in Section~\ref{S_eval_randomforest}, it yielded worse results than \textbf{RandomForest-UT} alone. We therefore inspected whether it could instead be used to find instances of the subtype \textit{unparsable}. This lead to a precision of 86.6\%, a recall of 87.9\%, and an F-Score of 86.3\%. A total of 87.9\% of the instances was classified correctly.

\newpage
\subsection{Detecting Datelines}
\label{s_datelines}
Source texts from news articles, like the ones in the TAC dataset, often contain datelines, which sometimes make it into the generated summary. While in the context of the article datelines make sense (often displayed smaller or otherwise visually distinctive), they are not needed, distracting and confusing in the summaries.

Datelines take many different forms, with some including a location. There is also a variety of appearing time formats, and some of them don't include timestamps at all:

\begin{framed}
\quad\textit{\textbf{BLACKSBURG, Virginia 2007-04-16 18:34: 44 UTC} A gunman opened fire in a dorm and classroom at Virginia Tech on Monday, killing at least 30 people in the deadliest shooting rampage in U.S. history.}

\quad\textit{\textbf{BERLIN, May 13( Xinhua)} The German government announced on Tuesday that it is to provide 500, 000 euros( around 770, 000 U.S. dollars) in aid for earthquake victims in Sichuan Province of China.}

\quad\textit{\textbf{00 a.m.}People are panicking.}
\end{framed}

\subsubsection{Method}
The patterns emerging through this violation are such that we can use regular expressions to detect them. A few iterations of the expression were created, based on the data in \textit{dev-2}, but so far the highest results were achieved using the regular expression

\begin{lstlisting}
UTC|^\d{4}-\d{2}-\d{2}|^[A-Z]{3,}|^(Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)
\end{lstlisting}

The expression has four parts, of which only one has to match to make the system classify a sentence as containing datelines:

\begin{itemize}
\item The first part matches the string ``UTC'' (Coordinated Universal Time, a common fragment in datelines)
\item The second one matches numeric dates like \textit{2014-04-13}
\item The third one matches capitalized words that are longer than 4 letters and are therefore expected not to be abbreviations (an example would be \textit{LONDON, Feb. 4 (Xinhua)})
\item And the final part matches abbreviated names of months.
\end{itemize}

Only ``UTC'' can occur anywhere, everything else needs to occur at the beginning of a sentence to be counted.

\subsubsection{Evaluation}
This very simple approach was evaluated on \textit{test} and leads to very promising results that can be seen in Table~\ref{eval_regex}. As a baseline, we again classified all sentences with the most-frequent class: not containing a dateline violation.

\begin{table}
\begin{center}
\begin{tabular}{|r|c|c|c|c|}
  \hline
  & \textbf{Precision} & \textbf{Recall} & \textbf{F-Score} & \textbf{Accuracy}\\
  \hline
  Baseline & - & 0 & - & 89.5\\
  \hline
  Regex & 86.0 & 89.7 & 87.8 & 97.4\\
  \hline
  \end{tabular}
\end{center}
\caption{Evaluation of \textbf{Regex} on \textit{test}.}
\label{eval_regex}
\end{table}

%dev2: 90.7\% (precision: 90.3\%, recall: 91.1\%)

When looking into the inaccurately detected instances, the inconsistencies of the LQVSumm corpus make up most of the false positives:

\begin{framed}
\textit{IVORY TUSKS}
\end{framed}

is classified as containing a dateline by the regular expression, but is not annotated as such in the corpus. An example of a false negative would be:

\begin{framed}
\textit{00 a.m. local time (0800 GMT)}
\end{framed}

which our system doesn't match.

\newpage
\subsection{Detecting Redundancy}
\label{redundancy}

Automatic summaries constructed from multiple source texts on the same topic often suffer from redundancy, as sentences that contain the same or largely the same information are occasionally included in the summary. \cite{friedrichlqvsumm} suggest this happens because redundant sentences are ranked highly with regards to content coverage.
As a consequence, reading fluency is impaired.

%While --- in contrast to many other linguistic quality violations --- this also has an impact on traditional evaluation scores for automatic summarization systems, it is also considered to be a problem of our domain.

In theory, redundancy should have a negative impact on traditional evaluation scores for automatic summarization systems, because the content coverage will be lower than it could be, since the space for the second redundant sentence could be replaced with another sentence without losing coverage. In practice though, content coverage correlates positively with occurence of redundancy \citep[see][]{friedrichlqvsumm} --- probably because content coverage of these sentences is higher than average.

In the corpus, redundancy is annotated by marking two phrases as redundant. Because we work on sentence level, we only consider redundancy violations whose first phrase starts and last phrase ends within two consecutive sentences.

In the LQVSumm corpus, different degrees of redundant sentence pairs occur.
Some sentence pairs annotated as redundant contain similar information, but using completely different phrasing and focus:

%low example
\begin{framed}
\textit{According to a survey by the State \textbf{Food and Drug} Administration, 65 percent of the respondents worried about the food \textbf{safety} situation in China.}

\textit{\textbf{Food and drug safety} has become a major concern of Chinese people.}
\end{framed}

Other pairs of sentences are apparently at least partly composed of the same source sentence, but are still different. One of the sentences might contain additional information and the two sentences might still be of very different length:

%medium example
\begin{framed}
\textit{\textbf{Cyclone} Sidr, described as the worst storm in years to hit low-lying and disaster-prone \textbf{Bangladesh}, crashed into \textbf{the southwestern coast Thursday} night \textbf{before sweeping north} over \textbf{the capital Dhaka.}}

\textit{The \textbf{cyclone} hit \†extbf{the southwestern coast} of \textbf{Bangladesh} on \textbf{Thursday before sweeping north} to \textbf{the capital Dhaka.}}
\end{framed}

Finally, sometimes subsequent sentences have marginal or even no differences:

%high example
\begin{framed}
\textit{\textbf{An unknown number of cats and dogs suffered kidney failure and about 10 died after eating the affected pet food, Menu Foods said in announcing the North American recall.}}

\textit{\textbf{An unknown number of cats and dogs suffered kidney failure, and about 10 died after eating the affected pet food, Menu Foods said in announcing the North American recall.}}
\end{framed}

We are seeking to find all of these sentences, but concentrate on the latter two cases, because detecting more complicated paraphrases is out of the scope of this thesis.% where the redundancy is obvious for humans.

\subsubsection{Method}
\label{s_unigrams}
%bigrams stemmed vs. unstemmed in table? No, no sign. diff.

Our method (\textbf{Unigrams}) consists of a simple unigram overlap score, which we found to outperform more sophisticated approaches:

\begin{enumerate}
\item First, we remove all non-alphanumeric characters from both sentences and split them into tokens
\item We then treat the words as a set and divide the cardinality of the intersection of each sentence's set by the number of possible overlaps, which will be the number of unique tokens in the shorter sentence.
\item A given threshold is then used to classify instances.
\end{enumerate}

Our methods are related to BLEU scores \citep{papineni2002bleu}, which we disregarded, because BLEU scores get their performance from having multiple reference translations available (which would be multiple redundant sentences in our usecase), which is not the case here.

A number of other methods are also implemented: \textbf{Bigrams} is a variation of the \textbf{Unigrams} method which uses bigrams instead of unigrams. \textbf{Combined} is using a score that consists of the arithmetic mean of the \textbf{Unigrams} and \textbf{Bigrams} scores. In \textbf{Bigrams}, the number of possible overlaps will be the number of unique bigrams in the shorter sentence.

\textbf{Levenshtein} consists of the standard edit distance \citep{levenshtein1966binary} on word level, normalized by the length of the shorter sentence. A threshold of 0.7 yielded best results on our development set, meaning that sentences are classified as containing redundancies if their \textit{normalized} word-level Levenshtein distance is smaller than 0.7. Prior to computation of the score, non-alphanumeric characters were again pruned from the sentences.
%\textbf{Levenshtein} consists of the standard edit distance \citep{levenshtein1966binary} on word level. A threshold of 2 yielded best results on our development set, meaning that sentences are classified as containing redundancies if their word-level Levenshtein distance is 1 or 0. Prior to computation of the score, non-alphanumeric characters were again pruned from the sentences.

Finally, as a further \textbf{Baseline} we classify all sentence pairs as not containing redundancies.

\subsubsection{Evaluation}

In the corpus, redundancy isn't annotated on sentence base, but rather on clause level, where ``clause'' is an arbitrary substring of the summary text. Redundancies are a pair of clauses which are connected with a redundancy relation.
These phrases can potentially be contained in a single sentence, or surround other, non-redundant phrases and sentences.

For our purposes, we only consider redundancies that occur in two subsequent sentences and our evaluation metrics are based on those.

To obtain a good threshold for \textbf{Unigrams}, \textbf{Bigrams}, and \textbf{Combined}, we compare their performance for various thresholds on the development sets \textit{dev-1} and \textit{dev-2}, from which we manually select the best threshold. The results can be seen in Figure~\ref{redundancy_graph}.

\begin{figure}
\begin{center}
\includegraphics[scale=0.15]{a.png}
\end{center}
\caption{Performance of various thresholds for \textbf{Unigrams}, \textbf{Bigrams}, and \textbf{Combined} in \textit{dev-1} and \textit{dev-2}. Continuous lines are recall, dashed lines are precision and bold lines are F-score.}
\label{redundancy_graph}
\end{figure}

For recall, \textbf{Unigrams} outperforms \textbf{Bigrams}, while for precision \textbf{Bigrams} is predominant.
As is to be expected, higher thresholds correlate with higher precision and lower recall. And while the F-scores are given in Figure~\ref{redundancy_graph} for informational purposes, they are not what we want to optimize; while recall is desirable, we feel that precision is much more important for a system that annotates violations, as explained in section \ref{experiments}.

As can be seen in Table~\ref{thresholds}, we therefore choose a threshold of $0.4$ for the \textbf{Bigrams} and \textbf{Combined}, and one of $0.5$ for \textbf{Unigrams}, because the precision-recall trade-off is optimal for our purpose: a useful recall, but still high precision.

The results, tested on \textit{test}, can be seen in Table~\ref{eval_redundancy}. Evidently, the \textbf{Unigrams} method outperforms \textbf{Levenshtein} and the \textbf{Baseline} as well as the other n-gram methods.

We unsuccessfully tried to increase our performance by a few means: Neither stemming the words with the Porter Stemming Algorithm \citep{porter1980algorithm} from NLTK \citep{bird2009natural}, nor part-of-speech-tagging the tokens with NLTK and only counting overlaps that include a verb (with the motivation of ruling out noun-based false positives) improved our methods. Looking up the first synset of each token in \textit{WordNet} \citep{fellbaum2005wordnet}, and building the intersections between the sets by considering two tokens to be the same if the first word is included in the second one's synset increased also didn't have any positive effects on our results.

\begin{table}
\begin{center}
\begin{tabular}{|c|c|c|}
\hline
\textbf{Unigrams} & \textbf{Bigrams} & \textbf{Combined} \\
\hline
0.5 & 0.4 & 0.4\\
\hline
\end{tabular}
\end{center}
\caption{Thresholds for \textbf{Unigrams}, \textbf{Bigrams}, and \textbf{Combined} determined on \textit{dev-1+2}}
\label{thresholds}
\end{table}

\begin{table}
\begin{center}
\begin{tabular}{|r|c|c|c|c|}
\hline
& Precision & Recall & F-Score & Accuracy\\
\hline
\textbf{Baseline} & - & 0\% & - & 95.5\% \\
\hline
%\textbf{Levenshtein} & 15.8\% & 17.3\% & 3.1\% \\
\textbf{Levenshtein} & 48.7\% & 11.0\% & 17.9 & 95.4\%\\
\hline
\textbf{Unigrams} & \textbf{58.0\%} & \textbf{28.2\%} & \textbf{37.0} & 94.7\%\\
\hline
\textbf{Bigrams} & 55.6\% & 14.5\% & 22.9 & 95.6\%\\
\hline
\textbf{Combined} & 56.8\% & 24.3\% & 34.0 & \textbf{95.7}\%\\
\hline
\end{tabular}
\end{center}
\caption{Evaluation of \textbf{Unigrams}, \textbf{Bigrams}, and \textbf{Combined} on \textit{test}}
\label{eval_redundancy}
\end{table}

% Bigrams: 0.5555555555555556,0.14450867052023122,0.22935779816513763
% Combined: 0.5675675675675675,0.24277456647398843,0.340080971659919
% Unigrams: 0.5802469135802469,0.27167630057803466,0.3700787401574803

% Levenshtein: 0.15789473684210526,0.017341040462427744,0.03125

% Baseline: 0.045240585774058575,1.0,0.08656492369276958

%\subsection{Unrelatedness} %evtl %NOOO
%\section{Evaluation} oder auch nicht
% or skip entirely and put in experiments? final results here?

\newpage
\section{Discussion}
\label{discussion}
% What seem to be the best methods for the various tasks and why?
Detecting linguistic quality violations automatically is not a trivial task. It is not always easy for humans to produce consistent annotations and annotations aren't necessarily highly consistent across humans, as can be seen in the inter-annotator agreement in \cite{friedrichlqvsumm}: Only \textit{incomplete sentence} and \textit{inclusion of datelines} have an agreement of over 90\%, annotators only agree on about 75\% of \textit{other ungrammaticality} violations and under 70\% for \textit{redundant information}. Part of that discrepancy can be attributed to the annotators' different experience level, but it might also indicate at least some subjectiveness.

Despite that, sometimes the easier methods outperformed the more sophisticated approaches.

In theory, \textbf{UnknownTokens-source} should return perfect results, with 100\% recall, precision and F-score. Some of this can probably be attributed to annotation mistakes during the annotation process of the subtypes. But manual inspection of the false positives showed that as the datelines (Section~\ref{s_datelines}) are not a part of the text of the source document (which we took the tokens from), geographical names only present in the datelines were wrongly assumed to be unknown tokens.

\textbf{UnknownTokens-gw+heur+ner+wiki}'s precision is weakened by a few words such as ``predisposes'' that were neither found in our Gigaword tokens, nor detected to be a named entity by either of our methods. The reader should keep in mind that because we are only evaluating this on \textit{dev-2} (as the subtypes are only annotated there) a couple of words can make a big difference.

Without finding evidence for this happening in our results, in theory cases could occur where a token is a known token (and rightly so), but can also be formed from two other tokens, when missing a space. This could not be correctly classified without contextual knowledge. A fictitious example could be:

\begin{framed}
\quad\textit{To try this taskmaster the other one first.}
\end{framed}

As taskmaster is an English word, it might occur in the list of known tokens, but will still be annotated as missing a space, because \textit{in this context} it is a contraction of \textit{task} and \textit{master}.
This means that achieving 100\% precision \textit{with this method} is theoretically impossible.

Our method of detecting datelines is close to the optimum achievable with a regular expression based approach without accidentally detecting times, time ranges or locations that are mentioned in the text as datelines.
When looking into the false positives, our decision to match abbreviations of month names leads to some sentences being classified as containing a dateline which actually do not contain one:

\begin{framed}
\quad\textit{\textbf{Mar}uca-Kovac said he phoned her at about 1 p.m. Wednesday, telling her he had left}
\end{framed}

% Other false positives could have been annotated as containing datelines in the corpus, but weren't:

% \quad\textit{\textbf{Nov}ember 17 itself fired a rocket against the US embassy in 1996}

Our biggest loss in terms of false negatives is that we don't detect datelines of the form:

\begin{framed}
\quad\textit{14( Xinhua)}
\end{framed}

Out of all of our best methods, our \textbf{Unigrams} method for detection of redundancies achieves the lowest F-score, because of our precision-recall trade-off.

Apart from that, one weakness of our approach is its simplicity: A token based method can only get so far when a subsequent sentence contains paraphrased content from the previous sentence.
\newpage

\begin{framed}
\quad\textit{\textbf{Defense Secretary Robert Gates} named an independent review panel \textbf{Friday} to investigate what he called an \underline{unacceptable situation'' in outpatient care} \textbf{at Walter Reed Army Medical Center}, \textbf{and he} said that some soldiers most directly involved'' in the problems \underline{have been removed from their positions.}}

\quad\textit{\textbf{Defense Secretary Robert} M. \textbf{Gates} said \textbf{Friday} that he would move swiftly to improve \underline{conditions} \underline{for wounded soldiers being treated as outpatients} \textbf{at Walter Reed Army Medical Center}, \textbf{and he} warned that senior officials \underline{would be held accountable for the poor conditions.}}
\end{framed}

While this example has some token overlap, the paraphrased content fails to get noticed by our system.

Another cause of a relatively low score is that in the LQVSumm corpus, redundancy annotations can be done on a smaller scale than what is classified with using our method. For example, a repeated phrase within two long sentences could be marked in the corpus, but our or similar approaches will not find these cases:

\begin{framed}
\quad\textit{\textbf{A gas well near Surabaya in East Java} operated by Lapindo Brantas Inc. has spewed steaming mud since May last year, submerging villages, industries and fields.}

\quad\textit{\textbf{A gas well near Surabaya in East Java} has spewed steaming mud since May last year, submerging villages, factories and fields and forcing more than 15,000 people to flee their homes.}
\end{framed}

The LQVSumm corpus that we used for all of our experiments is, unfortunately, not large enough to have a wide coverage of all violations. This can be seen most prominently when trying to detect redundancy (Section~\ref{redundancy}): While the corpus itself contains 504 \textit{redundant information} violations \citep[see][]{friedrichlqvsumm}, only 173 of them could be used by us, as we needed redundancies that occured in subsequent sentences and only within the bounds of those.
However, there's little to no alternatives available.

Apart from its size, the corpus is also sometimes annotated inconsistently or at least challenging for our purposes. In general, annotations are done on the phrase level, where phrases are arbitrary substrings of the summary. As we classify and evaluate on sentences produced by the sentence splitter of Stanford CoreNLP \citep{manning-EtAl:2014:P14-5}, this introduces noise. For our purposes, a corpus that had been segmented into sentences or claues before annotation would be preferable.

As can be seen in Section~\ref{s_subtypes}, the instances of \textit{other ungrammatical form} could have been divided into several types, which would help the automatic classification by allowing it to annotate them individually (which, for some cases, is an easier task).

%show example
A legitimate shortcoming of our work is that our methods, experiments, and results are tailored to the LQVSumm corpus and to the summarization systems that participated in the TAC 2011 Guided Summarization task \citep{owczarzak2011overview}.

\newpage
\section{Conclusion \& Future Work}
\label{conclusion}

In this paper, we developed detection methods for ungrammaticality, inclusion of datelines and redundancy, the results of which, evaluated on \textit{test}, can be seen in Table~8.

Along with this paper, the authors will publish a toolchain that can be used to annotate given documents automatically, with the best methods described in here.

We have presented a method to detect the kind of ungrammaticality that occurs in automatically generated extractive multi-document summaries. While the results are good, they do rely on a certain type of ungrammaticality occuring often (missing spaces). Future work could concentrate on improving other grammaticality violations, so that not only the accuracy rises, but also the generality. Such a system could potentially detect ungrammaticality in a much broader domain.

Another direction of research for future work could be finding other features for the classifier. Training a language model on a similar domain as the data (news texts, in our case) could be rewarding, as could exploiting the parser output more: We only use parser statistics, not the actual parse, from which other features could potentially be extracted. Another direction would be using an LFG parser instead of our HPSG parser, like \cite{wagner2007comparative} did.

We have also shown a simple but high-performing method of detecting datelines. We believe our results are close to what a human could achieve and don't see much room for improvement here, at least not with a variant of our method.

Finally we developed a basic method of detecting redundancy. With the lowest performance of all methods in this paper, redundancy detection is requiring much more work. While we based our approach only on the two subsequent sentences on which redundancy was to be tested, future work could include information available in the context of these sentences or even, as we did in \textbf{UnknownTokens-source}, information from the source documents.

One could also try following \cite{hatzivassiloglou1999detecting} closely and applying some form of machine learning as well, possibly using a similar setup to the one used in our \textbf{RandomForest} method.

\begin{table}[H]
\begin{center}
\begin{tabular}{|l|c|c|c|}
  \hline
   & Precision & Recall & F-Score\\
  \hline
  Ungrammaticality & 72.8 & 49.1 & 58.6\\
  \hline
  Datelines & 86.0 & 89.7 & 87.8\\
  \hline
  Redundancy & 58.0 & 28.2 & 37.0\\
  \hline
  \end{tabular}
\end{center}
\label{eval_best}
\caption{The three best methods for detecting Ungrammaticality (\textit{other ungrammatical form} + \textit{incomplete sentence}), Datelines (\textit{inclusion of datelines}) and Redundancy (\textit{redundant information})}
\end{table}

We only looked at a part of the violation types in \cite{friedrichlqvsumm}. Methods for detecting the last clause-level violation (no semantic relatedness), as well as the entity- and discourse-level violations have yet to be developed:

For the pronoun violations (\textit{pronoun with missing antecedent} and \textit{pronoun with misleading antecendent}), one could use the output of a coreference resolution tool such as BART \citep{versley2008bart} or the Stanford coreference resolution system \citep{lee2011stanford} to infer coreferences between entities in the summaries as well as the source documents and compare the originating coreference chains. If they match, the reference has probably been done correctly. If there's a mismatch, this points to a violation of one of the two pronoun violation types.

Finding \textit{acronym without explanation} seems to have a straightforward two-step solution: For every sequence of all-uppercase letters, find out whether those are an acronym that is not in a list of well-known acronyms. If it is, look for the expanded form of the acronym near its first occurence.

It is to be seen whether this method works as well as the author believes.

%TODO alles nochmal durchgehen

Going after the mention (\textit{first mention without explanation} and \textit{subsequent mention with explanation}) and noun phrase violations (\textit{definite noun phrase without reference to previous mention} and \textit{indefinite noun phrase with reference to previous mention}) would at least require a named entity recognition system, probably paired with at least some rudimentary form of semantic or discourse parsing.

A simpler approach of detecting \textit{indefinite noun phrase with reference to previous mention} could be to mark indefinite noun phrases whose head occurs earlier in the summary, although recall will presumably be rather low with this approach.

\textit{no semantic relatedness} also suggests an approach using semantic parsing, but usable results might even be achieved by some method based on the distance between content words in \textit{WordNet} \citep{fellbaum2005wordnet}.

Finally, the straightforward solution for detecting the \textit{no discourse relation} violation seems to be parsing the two sentences \textit{without} the discourse connective using a discourse parser, and then comparing the derived discourse relation with the relation commonly linked with the connective. For the last two mentioned types a corpus with more occurences of them would be essential.

Future work could also include annotating a corpus after segmenting it (into sentences or clauses), and using the adapted violation type system developed by us for that task.

And finally, since some of the characteristics of the violations our methods were tailored to are specific to the domain and corpus, it is yet to be seen how well our system performs on other corpora, other data sets and other domains.
Evaluation on anything other than extractive multi-document summarization systems however would probably lead to disappointing results: The violation types for which we have detection methods should not occur at all in abstractive systems, and some of them will likely not or only rarely occur in single-document summaries, for example \textit{redundant information} or some subtypes of \textit{other ungrammaticality}.

\newpage
\bibliographystyle{apalike}
\bibliography{referenzen}

\end{document}